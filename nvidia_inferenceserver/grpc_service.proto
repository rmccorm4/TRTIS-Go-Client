// Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions
// are met:
//  * Redistributions of source code must retain the above copyright
//    notice, this list of conditions and the following disclaimer.
//  * Redistributions in binary form must reproduce the above copyright
//    notice, this list of conditions and the following disclaimer in the
//    documentation and/or other materials provided with the distribution.
//  * Neither the name of NVIDIA CORPORATION nor the names of its
//    contributors may be used to endorse or promote products derived
//    from this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

syntax = "proto3";

package nvidia.inferenceserver;

//@@.. cpp:namespace:: nvidia::inferenceserver

import "api.proto";
import "request_status.proto";
import "server_status.proto";

//@@
//@@.. cpp:var:: service GRPCService
//@@
//@@   Inference Server GRPC endpoints.
//@@
service GRPCService
{
  //@@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
  //@@
  //@@     Get status for entire inference server or for a specified model.
  //@@
  rpc Status(StatusRequest) returns (StatusResponse) {}

  //@@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
  //@@
  //@@     Enable and disable low-level GPU profiling.
  //@@
  rpc Profile(ProfileRequest) returns (ProfileResponse) {}

  //@@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
  //@@
  //@@     Check liveness and readiness of the inference server.
  //@@
  rpc Health(HealthRequest) returns (HealthResponse) {}

  //@@  .. cpp:var:: rpc ModelControl(ModelControlRequest) returns
  //@@     (ModelControlResponse)
  //@@
  //@@     Request to load / unload a specified model.
  //@@
  rpc ModelControl(ModelControlRequest) returns (ModelControlResponse) {}

  //@@  .. cpp:var:: rpc SharedMemoryControl(SharedMemoryControlRequest) returns
  //@@     (SharedMemoryControlResponse)
  //@@
  //@@     Request to register / unregister a specified shared memory region.
  //@@
  rpc SharedMemoryControl(SharedMemoryControlRequest)
      returns (SharedMemoryControlResponse)
  {
  }

  //@@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
  //@@
  //@@     Request inference using a specific model. [ To handle large input
  //@@     tensors likely need to set the maximum message size to that they
  //@@     can be transmitted in one pass.
  //@@
  rpc Infer(InferRequest) returns (InferResponse) {}

  //@@  .. cpp:var:: rpc StreamInfer(stream InferRequest) returns (stream
  //@@     InferResponse)
  //@@
  //@@     Request inferences using a specific model in a streaming manner.
  //@@     Individual inference requests sent through the same stream will be
  //@@     processed in order and be returned on completion
  //@@
  rpc StreamInfer(stream InferRequest) returns (stream InferResponse) {}
}

//@@
//@@.. cpp:var:: message StatusRequest
//@@
//@@   Request message for Status gRPC endpoint.
//@@
message StatusRequest
{
  //@@
  //@@  .. cpp:var:: string model_name
  //@@
  //@@     The specific model status to be returned. If empty return status
  //@@     for all models.
  //@@
  string model_name = 1;
}

//@@
//@@.. cpp:var:: message StatusResponse
//@@
//@@   Response message for Status gRPC endpoint.
//@@
message StatusResponse
{
  //@@
  //@@  .. cpp:var:: RequestStatus request_status
  //@@
  //@@     The status of the request, indicating success or failure.
  //@@
  RequestStatus request_status = 1;

  //@@
  //@@  .. cpp:var:: ServerStatus server_status
  //@@
  //@@     The server and model status.
  //@@
  ServerStatus server_status = 2;
}

//@@
//@@.. cpp:var:: message ProfileRequest
//@@
//@@   Request message for Profile gRPC endpoint.
//@@
message ProfileRequest
{
  //@@
  //@@  .. cpp:var:: string cmd
  //@@
  //@@     The requested profiling action: 'start' requests that GPU
  //@@     profiling be enabled on all GPUs controlled by the inference
  //@@     server; 'stop' requests that GPU profiling be disabled on all GPUs
  //@@     controlled by the inference server.
  //@@
  string cmd = 1;
}

//@@
//@@.. cpp:var:: message ProfileResponse
//@@
//@@   Response message for Profile gRPC endpoint.
//@@
message ProfileResponse
{
  //@@
  //@@  .. cpp:var:: RequestStatus request_status
  //@@
  //@@     The status of the request, indicating success or failure.
  //@@
  RequestStatus request_status = 1;
}

//@@
//@@.. cpp:var:: message HealthRequest
//@@
//@@   Request message for Health gRPC endpoint.
//@@
message HealthRequest
{
  //@@
  //@@  .. cpp:var:: string mode
  //@@
  //@@     The requested health action: 'live' requests the liveness
  //@@     state of the inference server; 'ready' requests the readiness state
  //@@     of the inference server.
  //@@
  string mode = 1;
}

//@@
//@@.. cpp:var:: message HealthResponse
//@@
//@@   Response message for Health gRPC endpoint.
//@@
message HealthResponse
{
  //@@
  //@@  .. cpp:var:: RequestStatus request_status
  //@@
  //@@     The status of the request, indicating success or failure.
  //@@
  RequestStatus request_status = 1;

  //@@
  //@@  .. cpp:var:: bool health
  //@@
  //@@     The result of the request. True indicates the inference server is
  //@@     live/ready, false indicates the inference server is not live/ready.
  //@@
  bool health = 2;
}

//@@
//@@.. cpp:var:: message ModelControlRequest
//@@
//@@   Request message for ModelControl gRPC endpoint.
//@@
message ModelControlRequest
{
  //@@  .. cpp:enum:: Type
  //@@
  //@@     Types of control operation
  //@@
  enum Type {
    //@@    .. cpp:enumerator:: Type::UNLOAD = 0
    //@@
    //@@       To unload the specified model.
    //@@
    UNLOAD = 0;

    //@@    .. cpp:enumerator:: Type::LOAD = 1
    //@@
    //@@       To load the specified model. If the model has been loaded,
    //@@       it will be reloaded to fetch the latest change.
    //@@
    LOAD = 1;
  }

  //@@
  //@@  .. cpp:var:: string model_name
  //@@
  //@@     The target model name.
  //@@
  string model_name = 1;

  //@@
  //@@  .. cpp:var:: Type type
  //@@
  //@@     The control type that is operated on the specified model.
  //@@
  Type type = 2;
}

//@@
//@@.. cpp:var:: message ModelControlResponse
//@@
//@@   Response message for ModelControl gRPC endpoint.
//@@
message ModelControlResponse
{
  //@@
  //@@  .. cpp:var:: RequestStatus request_status
  //@@
  //@@     The status of the request, indicating success or failure.
  //@@
  RequestStatus request_status = 1;
}

//@@.. cpp:var:: message InferSharedMemoryRegion
//@@
//@@   The meta-data for the shared memory region from which to read the input
//@@   data and/or write the output data.
//@@
message InferSharedMemoryRegion
{
  //@@
  //@@  .. cpp:var:: string name
  //@@
  //@@     The name for this shared memory region.
  //@@
  string name = 1;

  //@@  .. cpp:var:: string shm_key
  //@@
  //@@     The name of the shared memory region that holds the input data
  //@@     (or where the output data should be written).
  //@@
  string shm_key = 2;

  //@@  .. cpp:var:: uint64 offset
  //@@
  //@@     The offset from the start of the shared memory region.
  //@@     start = offset, end = offset + size;
  //@@
  uint64 offset = 3;

  //@@  .. cpp:var:: uint64 byte_size
  //@@
  //@@     Size of the memory block, in bytes.
  //@@
  uint64 byte_size = 4;
}

//@@
//@@.. cpp:var:: message SharedMemoryControlRequest
//@@
//@@   Request message for managing registered shared memory regions in TRTIS.
//@@
message SharedMemoryControlRequest
{
  //@@  .. cpp:enum:: Type
  //@@
  //@@     Types of control operations for shared memory
  //@@
  enum Type {
    //@@    .. cpp:enumerator:: Type::REGISTER = 0
    //@@
    //@@       To register the specified shared memory region.
    //@@
    REGISTER = 0;

    //@@    .. cpp:enumerator:: Type::UNREGISTER = 1
    //@@
    //@@       To unregister the specified shared memory region.
    //@@
    UNREGISTER = 1;

    //@@    .. cpp:enumerator:: Type::UNREGISTER_ALL = 2
    //@@
    //@@       To unregister all active shared memory regions.
    //@@
    UNREGISTER_ALL = 2;
  }

  //@@
  //@@  .. cpp:var:: Type type
  //@@
  //@@     The control type that states whether to register/unregister the
  //@@     specified shared memory region. Unregister all active shared memory
  //@@     regions or get the list of active shared memory regions
  //@@
  Type type = 1;

  //@@  .. cpp:var:: InferSharedMemoryRegion shared_memory_region
  //@@
  //@@     The shared memory region to register or unregister.
  //@@     All fields are needed to REGISTER the shared memory region.
  //@@     Only 'name' is needed to UNREGISTER the shared memory region.
  //@@     No fields are needed to UNREGISTER_ALL.
  //@@
  InferSharedMemoryRegion shared_memory_region = 2;
}

//@@
//@@.. cpp:var:: message SharedMemoryControlResponse
//@@
//@@   Response message for SharedMemoryControl gRPC endpoint.
//@@
message SharedMemoryControlResponse
{
  //@@
  //@@  .. cpp:var:: RequestStatus request_status
  //@@
  //@@     The status of the request, indicating success or failure.
  //@@
  RequestStatus request_status = 1;
}

//@@
//@@.. cpp:var:: message InferRequest
//@@
//@@   Request message for Infer gRPC endpoint.
//@@
message InferRequest
{
  //@@  .. cpp:var:: string model_name
  //@@
  //@@     The name of the model to use for inferencing.
  //@@
  string model_name = 1;

  //@@  .. cpp:var:: int64 version
  //@@
  //@@     The version of the model to use for inference. If -1
  //@@     the latest/most-recent version of the model is used.
  //@@
  int64 model_version = 2;

  //@@  .. cpp:var:: InferRequestHeader meta_data
  //@@
  //@@     Meta-data for the request profiling input tensors and requesting
  //@@     output tensors.
  //@@
  InferRequestHeader meta_data = 3;

  //@@  .. cpp:var:: bytes raw_input (repeated)
  //@@
  //@@     The raw input tensor data in the order specified in 'meta_data'.
  //@@
  repeated bytes raw_input = 4;
}

//@@
//@@.. cpp:var:: message InferResponse
//@@
//@@   Response message for Infer gRPC endpoint.
//@@
message InferResponse
{
  //@@
  //@@  .. cpp:var:: RequestStatus request_status
  //@@
  //@@     The status of the request, indicating success or failure.
  //@@
  RequestStatus request_status = 1;

  //@@  .. cpp:var:: InferResponseHeader meta_data
  //@@
  //@@     The response meta-data for the output tensors.
  //@@
  InferResponseHeader meta_data = 2;

  //@@  .. cpp:var:: bytes raw_output (repeated)
  //@@
  //@@     The raw output tensor data in the order specified in 'meta_data'.
  //@@
  repeated bytes raw_output = 3;
}
