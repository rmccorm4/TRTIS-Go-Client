// Code generated by protoc-gen-go. DO NOT EDIT.
// source: model_config.proto

package nvidia_inferenceserver

import (
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

//@@
//@@.. cpp:enum:: DataType
//@@
//@@   Data types supported for input and output tensors.
//@@
type DataType int32

const (
	//@@  .. cpp:enumerator:: DataType::INVALID = 0
	DataType_TYPE_INVALID DataType = 0
	//@@  .. cpp:enumerator:: DataType::BOOL = 1
	DataType_TYPE_BOOL DataType = 1
	//@@  .. cpp:enumerator:: DataType::UINT8 = 2
	DataType_TYPE_UINT8 DataType = 2
	//@@  .. cpp:enumerator:: DataType::UINT16 = 3
	DataType_TYPE_UINT16 DataType = 3
	//@@  .. cpp:enumerator:: DataType::UINT32 = 4
	DataType_TYPE_UINT32 DataType = 4
	//@@  .. cpp:enumerator:: DataType::UINT64 = 5
	DataType_TYPE_UINT64 DataType = 5
	//@@  .. cpp:enumerator:: DataType::INT8 = 6
	DataType_TYPE_INT8 DataType = 6
	//@@  .. cpp:enumerator:: DataType::INT16 = 7
	DataType_TYPE_INT16 DataType = 7
	//@@  .. cpp:enumerator:: DataType::INT32 = 8
	DataType_TYPE_INT32 DataType = 8
	//@@  .. cpp:enumerator:: DataType::INT64 = 9
	DataType_TYPE_INT64 DataType = 9
	//@@  .. cpp:enumerator:: DataType::FP16 = 10
	DataType_TYPE_FP16 DataType = 10
	//@@  .. cpp:enumerator:: DataType::FP32 = 11
	DataType_TYPE_FP32 DataType = 11
	//@@  .. cpp:enumerator:: DataType::FP64 = 12
	DataType_TYPE_FP64 DataType = 12
	//@@  .. cpp:enumerator:: DataType::STRING = 13
	DataType_TYPE_STRING DataType = 13
)

var DataType_name = map[int32]string{
	0:  "TYPE_INVALID",
	1:  "TYPE_BOOL",
	2:  "TYPE_UINT8",
	3:  "TYPE_UINT16",
	4:  "TYPE_UINT32",
	5:  "TYPE_UINT64",
	6:  "TYPE_INT8",
	7:  "TYPE_INT16",
	8:  "TYPE_INT32",
	9:  "TYPE_INT64",
	10: "TYPE_FP16",
	11: "TYPE_FP32",
	12: "TYPE_FP64",
	13: "TYPE_STRING",
}

var DataType_value = map[string]int32{
	"TYPE_INVALID": 0,
	"TYPE_BOOL":    1,
	"TYPE_UINT8":   2,
	"TYPE_UINT16":  3,
	"TYPE_UINT32":  4,
	"TYPE_UINT64":  5,
	"TYPE_INT8":    6,
	"TYPE_INT16":   7,
	"TYPE_INT32":   8,
	"TYPE_INT64":   9,
	"TYPE_FP16":    10,
	"TYPE_FP32":    11,
	"TYPE_FP64":    12,
	"TYPE_STRING":  13,
}

func (x DataType) String() string {
	return proto.EnumName(DataType_name, int32(x))
}

func (DataType) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{0}
}

//@@
//@@  .. cpp:enum:: Kind
//@@
//@@     Kind of this instance group.
//@@
type ModelInstanceGroup_Kind int32

const (
	//@@    .. cpp:enumerator:: Kind::KIND_AUTO = 0
	//@@
	//@@       This instance group represents instances that can run on either
	//@@       CPU or GPU. If all GPUs listed in 'gpus' are available then
	//@@       instances will be created on GPU(s), otherwise instances will
	//@@       be created on CPU.
	//@@
	ModelInstanceGroup_KIND_AUTO ModelInstanceGroup_Kind = 0
	//@@    .. cpp:enumerator:: Kind::KIND_GPU = 1
	//@@
	//@@       This instance group represents instances that must run on the
	//@@       GPU.
	//@@
	ModelInstanceGroup_KIND_GPU ModelInstanceGroup_Kind = 1
	//@@    .. cpp:enumerator:: Kind::KIND_CPU = 2
	//@@
	//@@       This instance group represents instances that must run on the
	//@@       CPU.
	//@@
	ModelInstanceGroup_KIND_CPU ModelInstanceGroup_Kind = 2
	//@@    .. cpp:enumerator:: Kind::KIND_MODEL = 3
	//@@
	//@@       This instance group represents instances that should run on the
	//@@       CPU and/or GPU(s) as specified by the model or backend itself.
	//@@       The inference server will not override the model/backend
	//@@       settings.
	//@@       Currently, this option is supported only for Tensorflow models.
	//@@
	ModelInstanceGroup_KIND_MODEL ModelInstanceGroup_Kind = 3
)

var ModelInstanceGroup_Kind_name = map[int32]string{
	0: "KIND_AUTO",
	1: "KIND_GPU",
	2: "KIND_CPU",
	3: "KIND_MODEL",
}

var ModelInstanceGroup_Kind_value = map[string]int32{
	"KIND_AUTO":  0,
	"KIND_GPU":   1,
	"KIND_CPU":   2,
	"KIND_MODEL": 3,
}

func (x ModelInstanceGroup_Kind) String() string {
	return proto.EnumName(ModelInstanceGroup_Kind_name, int32(x))
}

func (ModelInstanceGroup_Kind) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{0, 0}
}

//@@
//@@  .. cpp:enum:: Format
//@@
//@@     The format for the input.
//@@
type ModelInput_Format int32

const (
	//@@    .. cpp:enumerator:: Format::FORMAT_NONE = 0
	//@@
	//@@       The input has no specific format. This is the default.
	//@@
	ModelInput_FORMAT_NONE ModelInput_Format = 0
	//@@    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
	//@@
	//@@       HWC image format. Tensors with this format require 3 dimensions
	//@@       if the model does not support batching (max_batch_size = 0) or 4
	//@@       dimensions if the model does support batching (max_batch_size
	//@@       >= 1). In either case the 'dims' below should only specify the
	//@@       3 non-batch dimensions (i.e. HWC or CHW).
	//@@
	ModelInput_FORMAT_NHWC ModelInput_Format = 1
	//@@    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
	//@@
	//@@       CHW image format. Tensors with this format require 3 dimensions
	//@@       if the model does not support batching (max_batch_size = 0) or 4
	//@@       dimensions if the model does support batching (max_batch_size
	//@@       >= 1). In either case the 'dims' below should only specify the
	//@@       3 non-batch dimensions (i.e. HWC or CHW).
	//@@
	ModelInput_FORMAT_NCHW ModelInput_Format = 2
)

var ModelInput_Format_name = map[int32]string{
	0: "FORMAT_NONE",
	1: "FORMAT_NHWC",
	2: "FORMAT_NCHW",
}

var ModelInput_Format_value = map[string]int32{
	"FORMAT_NONE": 0,
	"FORMAT_NHWC": 1,
	"FORMAT_NCHW": 2,
}

func (x ModelInput_Format) String() string {
	return proto.EnumName(ModelInput_Format_name, int32(x))
}

func (ModelInput_Format) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{2, 0}
}

//@@
//@@  .. cpp:enum:: ModelPriority
//@@
//@@     Model priorities. A model will be given scheduling and execution
//@@     preference over models at lower priorities. Current model
//@@     priorities only work for TensorRT models.
//@@
type ModelOptimizationPolicy_ModelPriority int32

const (
	//@@    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
	//@@
	//@@       The default model priority.
	//@@
	ModelOptimizationPolicy_PRIORITY_DEFAULT ModelOptimizationPolicy_ModelPriority = 0
	//@@    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
	//@@
	//@@       The maximum model priority.
	//@@
	ModelOptimizationPolicy_PRIORITY_MAX ModelOptimizationPolicy_ModelPriority = 1
	//@@    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
	//@@
	//@@       The minimum model priority.
	//@@
	ModelOptimizationPolicy_PRIORITY_MIN ModelOptimizationPolicy_ModelPriority = 2
)

var ModelOptimizationPolicy_ModelPriority_name = map[int32]string{
	0: "PRIORITY_DEFAULT",
	1: "PRIORITY_MAX",
	2: "PRIORITY_MIN",
}

var ModelOptimizationPolicy_ModelPriority_value = map[string]int32{
	"PRIORITY_DEFAULT": 0,
	"PRIORITY_MAX":     1,
	"PRIORITY_MIN":     2,
}

func (x ModelOptimizationPolicy_ModelPriority) String() string {
	return proto.EnumName(ModelOptimizationPolicy_ModelPriority_name, int32(x))
}

func (ModelOptimizationPolicy_ModelPriority) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 0}
}

//@@
//@@    .. cpp:enum:: Kind
//@@
//@@       The kind of the control.
//@@
type ModelSequenceBatching_Control_Kind int32

const (
	//@@      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
	//@@
	//@@         A new sequence is/is-not starting. If true a sequence is
	//@@         starting, if false a sequence is continuing.
	//@@
	ModelSequenceBatching_Control_CONTROL_SEQUENCE_START ModelSequenceBatching_Control_Kind = 0
	//@@      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
	//@@
	//@@         A sequence is/is-not ready for inference. If true the
	//@@         input tensor data is valid and should be used. If false
	//@@         the input tensor data is invalid and inferencing should
	//@@         be "skipped".
	//@@
	ModelSequenceBatching_Control_CONTROL_SEQUENCE_READY ModelSequenceBatching_Control_Kind = 1
)

var ModelSequenceBatching_Control_Kind_name = map[int32]string{
	0: "CONTROL_SEQUENCE_START",
	1: "CONTROL_SEQUENCE_READY",
}

var ModelSequenceBatching_Control_Kind_value = map[string]int32{
	"CONTROL_SEQUENCE_START": 0,
	"CONTROL_SEQUENCE_READY": 1,
}

func (x ModelSequenceBatching_Control_Kind) String() string {
	return proto.EnumName(ModelSequenceBatching_Control_Kind_name, int32(x))
}

func (ModelSequenceBatching_Control_Kind) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{7, 0, 0}
}

//@@
//@@.. cpp:var:: message ModelInstanceGroup
//@@
//@@   A group of one or more instances of a model and resources made
//@@   available for those instances.
//@@
type ModelInstanceGroup struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     Optional name of this group of instances. If not specified the
	//@@     name will be formed as <model name>_<group number>. The name of
	//@@     individual instances will be further formed by a unique instance
	//@@     number and GPU index:
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: Kind kind
	//@@
	//@@     The kind of this instance group. Default is KIND_AUTO. If
	//@@     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
	//@@     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
	//@@     and 'gpu' cannot be specified.
	//@@
	Kind ModelInstanceGroup_Kind `protobuf:"varint,4,opt,name=kind,proto3,enum=nvidia.inferenceserver.ModelInstanceGroup_Kind" json:"kind,omitempty"`
	//@@  .. cpp:var:: int32 count
	//@@
	//@@     For a group assigned to GPU, the number of instances created for
	//@@     each GPU listed in 'gpus'. For a group assigned to CPU the number
	//@@     of instances created. Default is 1.
	Count int32 `protobuf:"varint,2,opt,name=count,proto3" json:"count,omitempty"`
	//@@  .. cpp:var:: int32 gpus (repeated)
	//@@
	//@@     GPU(s) where instances should be available. For each GPU listed,
	//@@     'count' instances of the model will be available. Setting 'gpus'
	//@@     to empty (or not specifying at all) is eqivalent to listing all
	//@@     available GPUs.
	//@@
	Gpus                 []int32  `protobuf:"varint,3,rep,packed,name=gpus,proto3" json:"gpus,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInstanceGroup) Reset()         { *m = ModelInstanceGroup{} }
func (m *ModelInstanceGroup) String() string { return proto.CompactTextString(m) }
func (*ModelInstanceGroup) ProtoMessage()    {}
func (*ModelInstanceGroup) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{0}
}

func (m *ModelInstanceGroup) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInstanceGroup.Unmarshal(m, b)
}
func (m *ModelInstanceGroup) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInstanceGroup.Marshal(b, m, deterministic)
}
func (m *ModelInstanceGroup) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInstanceGroup.Merge(m, src)
}
func (m *ModelInstanceGroup) XXX_Size() int {
	return xxx_messageInfo_ModelInstanceGroup.Size(m)
}
func (m *ModelInstanceGroup) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInstanceGroup.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInstanceGroup proto.InternalMessageInfo

func (m *ModelInstanceGroup) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInstanceGroup) GetKind() ModelInstanceGroup_Kind {
	if m != nil {
		return m.Kind
	}
	return ModelInstanceGroup_KIND_AUTO
}

func (m *ModelInstanceGroup) GetCount() int32 {
	if m != nil {
		return m.Count
	}
	return 0
}

func (m *ModelInstanceGroup) GetGpus() []int32 {
	if m != nil {
		return m.Gpus
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelTensorReshape
//@@
//@@   Reshape specification for input and output tensors.
//@@
type ModelTensorReshape struct {
	//@@  .. cpp:var:: int64 shape (repeated)
	//@@
	//@@     The shape to use for reshaping.
	//@@
	Shape                []int64  `protobuf:"varint,1,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelTensorReshape) Reset()         { *m = ModelTensorReshape{} }
func (m *ModelTensorReshape) String() string { return proto.CompactTextString(m) }
func (*ModelTensorReshape) ProtoMessage()    {}
func (*ModelTensorReshape) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{1}
}

func (m *ModelTensorReshape) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelTensorReshape.Unmarshal(m, b)
}
func (m *ModelTensorReshape) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelTensorReshape.Marshal(b, m, deterministic)
}
func (m *ModelTensorReshape) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelTensorReshape.Merge(m, src)
}
func (m *ModelTensorReshape) XXX_Size() int {
	return xxx_messageInfo_ModelTensorReshape.Size(m)
}
func (m *ModelTensorReshape) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelTensorReshape.DiscardUnknown(m)
}

var xxx_messageInfo_ModelTensorReshape proto.InternalMessageInfo

func (m *ModelTensorReshape) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelInput
//@@
//@@   An input required by the model.
//@@
type ModelInput struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the input.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: DataType data_type
	//@@
	//@@     The data-type of the input.
	//@@
	DataType DataType `protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=nvidia.inferenceserver.DataType" json:"data_type,omitempty"`
	//@@  .. cpp:var:: Format format
	//@@
	//@@     The format of the input. Optional.
	//@@
	Format ModelInput_Format `protobuf:"varint,3,opt,name=format,proto3,enum=nvidia.inferenceserver.ModelInput_Format" json:"format,omitempty"`
	//@@  .. cpp:var:: int64 dims (repeated)
	//@@
	//@@     The dimensions/shape of the input tensor that must be provided
	//@@     when invoking the inference API for this model.
	//@@
	Dims []int64 `protobuf:"varint,4,rep,packed,name=dims,proto3" json:"dims,omitempty"`
	//@@  .. cpp:var:: ModelTensorReshape reshape
	//@@
	//@@     The shape expected for this input by the backend. The input will
	//@@     be reshaped to this before being presented to the backend. The
	//@@     reshape must have the same number of elements as the input shape
	//@@     specified by 'dims'. Optional.
	//@@
	Reshape              *ModelTensorReshape `protobuf:"bytes,5,opt,name=reshape,proto3" json:"reshape,omitempty"`
	XXX_NoUnkeyedLiteral struct{}            `json:"-"`
	XXX_unrecognized     []byte              `json:"-"`
	XXX_sizecache        int32               `json:"-"`
}

func (m *ModelInput) Reset()         { *m = ModelInput{} }
func (m *ModelInput) String() string { return proto.CompactTextString(m) }
func (*ModelInput) ProtoMessage()    {}
func (*ModelInput) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{2}
}

func (m *ModelInput) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInput.Unmarshal(m, b)
}
func (m *ModelInput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInput.Marshal(b, m, deterministic)
}
func (m *ModelInput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInput.Merge(m, src)
}
func (m *ModelInput) XXX_Size() int {
	return xxx_messageInfo_ModelInput.Size(m)
}
func (m *ModelInput) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInput.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInput proto.InternalMessageInfo

func (m *ModelInput) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInput) GetDataType() DataType {
	if m != nil {
		return m.DataType
	}
	return DataType_TYPE_INVALID
}

func (m *ModelInput) GetFormat() ModelInput_Format {
	if m != nil {
		return m.Format
	}
	return ModelInput_FORMAT_NONE
}

func (m *ModelInput) GetDims() []int64 {
	if m != nil {
		return m.Dims
	}
	return nil
}

func (m *ModelInput) GetReshape() *ModelTensorReshape {
	if m != nil {
		return m.Reshape
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelOutput
//@@
//@@   An output produced by the model.
//@@
type ModelOutput struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the output.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: DataType data_type
	//@@
	//@@     The data-type of the output.
	//@@
	DataType DataType `protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=nvidia.inferenceserver.DataType" json:"data_type,omitempty"`
	//@@  .. cpp:var:: int64 dims (repeated)
	//@@
	//@@     The dimensions/shape of the output tensor.
	//@@
	Dims []int64 `protobuf:"varint,3,rep,packed,name=dims,proto3" json:"dims,omitempty"`
	//@@  .. cpp:var:: ModelTensorReshape reshape
	//@@
	//@@     The shape produced for this output by the backend. The output will
	//@@     be reshaped from this to the shape specifed in 'dims' before being
	//@@     returned in the inference response. The reshape must have the same
	//@@     number of elements as the output shape specified by 'dims'. Optional.
	//@@
	Reshape *ModelTensorReshape `protobuf:"bytes,5,opt,name=reshape,proto3" json:"reshape,omitempty"`
	//@@  .. cpp:var:: string label_filename
	//@@
	//@@     The label file associated with this output. Should be specified only
	//@@     for outputs that represent classifications. Optional.
	//@@
	LabelFilename        string   `protobuf:"bytes,4,opt,name=label_filename,json=labelFilename,proto3" json:"label_filename,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOutput) Reset()         { *m = ModelOutput{} }
func (m *ModelOutput) String() string { return proto.CompactTextString(m) }
func (*ModelOutput) ProtoMessage()    {}
func (*ModelOutput) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{3}
}

func (m *ModelOutput) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOutput.Unmarshal(m, b)
}
func (m *ModelOutput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOutput.Marshal(b, m, deterministic)
}
func (m *ModelOutput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOutput.Merge(m, src)
}
func (m *ModelOutput) XXX_Size() int {
	return xxx_messageInfo_ModelOutput.Size(m)
}
func (m *ModelOutput) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOutput.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOutput proto.InternalMessageInfo

func (m *ModelOutput) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelOutput) GetDataType() DataType {
	if m != nil {
		return m.DataType
	}
	return DataType_TYPE_INVALID
}

func (m *ModelOutput) GetDims() []int64 {
	if m != nil {
		return m.Dims
	}
	return nil
}

func (m *ModelOutput) GetReshape() *ModelTensorReshape {
	if m != nil {
		return m.Reshape
	}
	return nil
}

func (m *ModelOutput) GetLabelFilename() string {
	if m != nil {
		return m.LabelFilename
	}
	return ""
}

//@@
//@@.. cpp:var:: message ModelVersionPolicy
//@@
//@@   Policy indicating which versions of a model should be made
//@@   available by the inference server.
//@@
type ModelVersionPolicy struct {
	//@@  .. cpp:var:: oneof policy_choice
	//@@
	//@@     Each model must implement only a single version policy. The
	//@@     default policy is 'Latest'.
	//@@
	//
	// Types that are valid to be assigned to PolicyChoice:
	//	*ModelVersionPolicy_Latest_
	//	*ModelVersionPolicy_All_
	//	*ModelVersionPolicy_Specific_
	PolicyChoice         isModelVersionPolicy_PolicyChoice `protobuf_oneof:"policy_choice"`
	XXX_NoUnkeyedLiteral struct{}                          `json:"-"`
	XXX_unrecognized     []byte                            `json:"-"`
	XXX_sizecache        int32                             `json:"-"`
}

func (m *ModelVersionPolicy) Reset()         { *m = ModelVersionPolicy{} }
func (m *ModelVersionPolicy) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy) ProtoMessage()    {}
func (*ModelVersionPolicy) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4}
}

func (m *ModelVersionPolicy) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy.Unmarshal(m, b)
}
func (m *ModelVersionPolicy) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy.Merge(m, src)
}
func (m *ModelVersionPolicy) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy.Size(m)
}
func (m *ModelVersionPolicy) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy proto.InternalMessageInfo

type isModelVersionPolicy_PolicyChoice interface {
	isModelVersionPolicy_PolicyChoice()
}

type ModelVersionPolicy_Latest_ struct {
	Latest *ModelVersionPolicy_Latest `protobuf:"bytes,1,opt,name=latest,proto3,oneof"`
}

type ModelVersionPolicy_All_ struct {
	All *ModelVersionPolicy_All `protobuf:"bytes,2,opt,name=all,proto3,oneof"`
}

type ModelVersionPolicy_Specific_ struct {
	Specific *ModelVersionPolicy_Specific `protobuf:"bytes,3,opt,name=specific,proto3,oneof"`
}

func (*ModelVersionPolicy_Latest_) isModelVersionPolicy_PolicyChoice() {}

func (*ModelVersionPolicy_All_) isModelVersionPolicy_PolicyChoice() {}

func (*ModelVersionPolicy_Specific_) isModelVersionPolicy_PolicyChoice() {}

func (m *ModelVersionPolicy) GetPolicyChoice() isModelVersionPolicy_PolicyChoice {
	if m != nil {
		return m.PolicyChoice
	}
	return nil
}

func (m *ModelVersionPolicy) GetLatest() *ModelVersionPolicy_Latest {
	if x, ok := m.GetPolicyChoice().(*ModelVersionPolicy_Latest_); ok {
		return x.Latest
	}
	return nil
}

func (m *ModelVersionPolicy) GetAll() *ModelVersionPolicy_All {
	if x, ok := m.GetPolicyChoice().(*ModelVersionPolicy_All_); ok {
		return x.All
	}
	return nil
}

func (m *ModelVersionPolicy) GetSpecific() *ModelVersionPolicy_Specific {
	if x, ok := m.GetPolicyChoice().(*ModelVersionPolicy_Specific_); ok {
		return x.Specific
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*ModelVersionPolicy) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*ModelVersionPolicy_Latest_)(nil),
		(*ModelVersionPolicy_All_)(nil),
		(*ModelVersionPolicy_Specific_)(nil),
	}
}

//@@  .. cpp:var:: message Latest
//@@
//@@     Serve only the latest version(s) of a model. This is
//@@     the default policy.
//@@
type ModelVersionPolicy_Latest struct {
	//@@    .. cpp:var:: uint32 num_versions
	//@@
	//@@       Serve only the 'num_versions' highest-numbered versions. T
	//@@       The default value of 'num_versions' is 1, indicating that by
	//@@       default only the single highest-number version of a
	//@@       model will be served.
	//@@
	NumVersions          uint32   `protobuf:"varint,1,opt,name=num_versions,json=numVersions,proto3" json:"num_versions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelVersionPolicy_Latest) Reset()         { *m = ModelVersionPolicy_Latest{} }
func (m *ModelVersionPolicy_Latest) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy_Latest) ProtoMessage()    {}
func (*ModelVersionPolicy_Latest) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4, 0}
}

func (m *ModelVersionPolicy_Latest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy_Latest.Unmarshal(m, b)
}
func (m *ModelVersionPolicy_Latest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy_Latest.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy_Latest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy_Latest.Merge(m, src)
}
func (m *ModelVersionPolicy_Latest) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy_Latest.Size(m)
}
func (m *ModelVersionPolicy_Latest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy_Latest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy_Latest proto.InternalMessageInfo

func (m *ModelVersionPolicy_Latest) GetNumVersions() uint32 {
	if m != nil {
		return m.NumVersions
	}
	return 0
}

//@@  .. cpp:var:: message All
//@@
//@@     Serve all versions of the model.
//@@
type ModelVersionPolicy_All struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelVersionPolicy_All) Reset()         { *m = ModelVersionPolicy_All{} }
func (m *ModelVersionPolicy_All) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy_All) ProtoMessage()    {}
func (*ModelVersionPolicy_All) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4, 1}
}

func (m *ModelVersionPolicy_All) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy_All.Unmarshal(m, b)
}
func (m *ModelVersionPolicy_All) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy_All.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy_All) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy_All.Merge(m, src)
}
func (m *ModelVersionPolicy_All) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy_All.Size(m)
}
func (m *ModelVersionPolicy_All) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy_All.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy_All proto.InternalMessageInfo

//@@  .. cpp:var:: message Specific
//@@
//@@     Serve only specific versions of the model.
//@@
type ModelVersionPolicy_Specific struct {
	//@@    .. cpp:var:: int64 versions (repeated)
	//@@
	//@@       The specific versions of the model that will be served.
	//@@
	Versions             []int64  `protobuf:"varint,1,rep,packed,name=versions,proto3" json:"versions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelVersionPolicy_Specific) Reset()         { *m = ModelVersionPolicy_Specific{} }
func (m *ModelVersionPolicy_Specific) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy_Specific) ProtoMessage()    {}
func (*ModelVersionPolicy_Specific) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4, 2}
}

func (m *ModelVersionPolicy_Specific) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy_Specific.Unmarshal(m, b)
}
func (m *ModelVersionPolicy_Specific) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy_Specific.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy_Specific) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy_Specific.Merge(m, src)
}
func (m *ModelVersionPolicy_Specific) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy_Specific.Size(m)
}
func (m *ModelVersionPolicy_Specific) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy_Specific.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy_Specific proto.InternalMessageInfo

func (m *ModelVersionPolicy_Specific) GetVersions() []int64 {
	if m != nil {
		return m.Versions
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelOptimizationPolicy
//@@
//@@   Optimization settings for a model. These settings control if/how a
//@@   model is optimized and prioritized by the backend framework when
//@@   it is loaded.
//@@
type ModelOptimizationPolicy struct {
	//@@  .. cpp:var:: Graph graph
	//@@
	//@@     The graph optimization setting for the model. Optional.
	//@@
	Graph *ModelOptimizationPolicy_Graph `protobuf:"bytes,1,opt,name=graph,proto3" json:"graph,omitempty"`
	//@@  .. cpp:var:: ModelPriority priority
	//@@
	//@@     The priority setting for the model. Optional.
	//@@
	Priority ModelOptimizationPolicy_ModelPriority `protobuf:"varint,2,opt,name=priority,proto3,enum=nvidia.inferenceserver.ModelOptimizationPolicy_ModelPriority" json:"priority,omitempty"`
	//@@  .. cpp:var:: Cuda cuda
	//@@
	//@@     CUDA-specific optimization settings. Optional.
	//@@
	Cuda                 *ModelOptimizationPolicy_Cuda `protobuf:"bytes,3,opt,name=cuda,proto3" json:"cuda,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                      `json:"-"`
	XXX_unrecognized     []byte                        `json:"-"`
	XXX_sizecache        int32                         `json:"-"`
}

func (m *ModelOptimizationPolicy) Reset()         { *m = ModelOptimizationPolicy{} }
func (m *ModelOptimizationPolicy) String() string { return proto.CompactTextString(m) }
func (*ModelOptimizationPolicy) ProtoMessage()    {}
func (*ModelOptimizationPolicy) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5}
}

func (m *ModelOptimizationPolicy) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy.Merge(m, src)
}
func (m *ModelOptimizationPolicy) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy.Size(m)
}
func (m *ModelOptimizationPolicy) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy proto.InternalMessageInfo

func (m *ModelOptimizationPolicy) GetGraph() *ModelOptimizationPolicy_Graph {
	if m != nil {
		return m.Graph
	}
	return nil
}

func (m *ModelOptimizationPolicy) GetPriority() ModelOptimizationPolicy_ModelPriority {
	if m != nil {
		return m.Priority
	}
	return ModelOptimizationPolicy_PRIORITY_DEFAULT
}

func (m *ModelOptimizationPolicy) GetCuda() *ModelOptimizationPolicy_Cuda {
	if m != nil {
		return m.Cuda
	}
	return nil
}

//@@
//@@  .. cpp:var:: message Graph
//@@
//@@     Enable generic graph optimization of the model. If not specified
//@@     the framework's default level of optimization is used. Currently
//@@     only supported for TensorFlow graphdef and savedmodel models and
//@@     causes XLA to be enabled/disabled for the model.
//@@
type ModelOptimizationPolicy_Graph struct {
	//@@    .. cpp:var:: int32 level
	//@@
	//@@       The optimization level. Defaults to 0 (zero) if not specified.
	//@@
	//@@         - -1: Disabled
	//@@         -  0: Framework default
	//@@         -  1+: Enable optimization level (greater values indicate
	//@@            higher optimization levels)
	//@@
	Level                int32    `protobuf:"varint,1,opt,name=level,proto3" json:"level,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOptimizationPolicy_Graph) Reset()         { *m = ModelOptimizationPolicy_Graph{} }
func (m *ModelOptimizationPolicy_Graph) String() string { return proto.CompactTextString(m) }
func (*ModelOptimizationPolicy_Graph) ProtoMessage()    {}
func (*ModelOptimizationPolicy_Graph) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 0}
}

func (m *ModelOptimizationPolicy_Graph) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_Graph.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_Graph) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_Graph.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_Graph) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_Graph.Merge(m, src)
}
func (m *ModelOptimizationPolicy_Graph) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_Graph.Size(m)
}
func (m *ModelOptimizationPolicy_Graph) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_Graph.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_Graph proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_Graph) GetLevel() int32 {
	if m != nil {
		return m.Level
	}
	return 0
}

//@@
//@@  .. cpp:var:: message Cuda
//@@
//@@     CUDA-specific optimization settings.
//@@
type ModelOptimizationPolicy_Cuda struct {
	//@@    .. cpp:var:: bool graphs
	//@@
	//@@       Use CUDA graphs API to capture model operations and execute
	//@@       them more efficiently. Currently only recognized by TensorRT
	//@@       backend.
	//@@
	Graphs               bool     `protobuf:"varint,1,opt,name=graphs,proto3" json:"graphs,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOptimizationPolicy_Cuda) Reset()         { *m = ModelOptimizationPolicy_Cuda{} }
func (m *ModelOptimizationPolicy_Cuda) String() string { return proto.CompactTextString(m) }
func (*ModelOptimizationPolicy_Cuda) ProtoMessage()    {}
func (*ModelOptimizationPolicy_Cuda) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 1}
}

func (m *ModelOptimizationPolicy_Cuda) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_Cuda.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_Cuda.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_Cuda.Merge(m, src)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_Cuda.Size(m)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_Cuda.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_Cuda proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_Cuda) GetGraphs() bool {
	if m != nil {
		return m.Graphs
	}
	return false
}

//@@
//@@.. cpp:var:: message ModelDynamicBatching
//@@
//@@   Dynamic batching configuration. These settings control how dynamic
//@@   batching operates for the model.
//@@
type ModelDynamicBatching struct {
	//@@  .. cpp:var:: int32 preferred_batch_size (repeated)
	//@@
	//@@     Preferred batch sizes for dynamic batching. If a batch of one of
	//@@     these sizes can be formed it will be executed immediately.  If
	//@@     not specified a preferred batch size will be chosen automatically
	//@@     based on model and GPU characteristics.
	//@@
	PreferredBatchSize []int32 `protobuf:"varint,1,rep,packed,name=preferred_batch_size,json=preferredBatchSize,proto3" json:"preferred_batch_size,omitempty"`
	//@@  .. cpp:var:: uint64 max_queue_delay_microseconds
	//@@
	//@@     The maximum time, in microseconds, a request will be delayed in
	//@@     the scheduling queue to wait for additional requests for
	//@@     batching. Default is 0.
	//@@
	MaxQueueDelayMicroseconds uint64   `protobuf:"varint,2,opt,name=max_queue_delay_microseconds,json=maxQueueDelayMicroseconds,proto3" json:"max_queue_delay_microseconds,omitempty"`
	XXX_NoUnkeyedLiteral      struct{} `json:"-"`
	XXX_unrecognized          []byte   `json:"-"`
	XXX_sizecache             int32    `json:"-"`
}

func (m *ModelDynamicBatching) Reset()         { *m = ModelDynamicBatching{} }
func (m *ModelDynamicBatching) String() string { return proto.CompactTextString(m) }
func (*ModelDynamicBatching) ProtoMessage()    {}
func (*ModelDynamicBatching) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{6}
}

func (m *ModelDynamicBatching) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelDynamicBatching.Unmarshal(m, b)
}
func (m *ModelDynamicBatching) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelDynamicBatching.Marshal(b, m, deterministic)
}
func (m *ModelDynamicBatching) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelDynamicBatching.Merge(m, src)
}
func (m *ModelDynamicBatching) XXX_Size() int {
	return xxx_messageInfo_ModelDynamicBatching.Size(m)
}
func (m *ModelDynamicBatching) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelDynamicBatching.DiscardUnknown(m)
}

var xxx_messageInfo_ModelDynamicBatching proto.InternalMessageInfo

func (m *ModelDynamicBatching) GetPreferredBatchSize() []int32 {
	if m != nil {
		return m.PreferredBatchSize
	}
	return nil
}

func (m *ModelDynamicBatching) GetMaxQueueDelayMicroseconds() uint64 {
	if m != nil {
		return m.MaxQueueDelayMicroseconds
	}
	return 0
}

//@@
//@@.. cpp:var:: message ModelSequenceBatching
//@@
//@@   Sequence batching configuration. These settings control how sequence
//@@   batching operates for the model.
//@@
type ModelSequenceBatching struct {
	//@@  .. cpp:var:: uint64 max_sequence_idle_microseconds
	//@@
	//@@     The maximum time, in microseconds, that a sequence is allowed to
	//@@     be idle before it is aborted. The inference server considers a
	//@@     sequence idle when it does not have any inference request queued
	//@@     for the sequence. If this limit is exceeded, the inference server
	//@@     will free the batch slot allocated by the sequence and make it
	//@@     available for another sequence. If not specified (or specified as
	//@@     zero) a default value of 1000000 (1 second) is used.
	//@@
	MaxSequenceIdleMicroseconds uint64 `protobuf:"varint,1,opt,name=max_sequence_idle_microseconds,json=maxSequenceIdleMicroseconds,proto3" json:"max_sequence_idle_microseconds,omitempty"`
	//@@  .. cpp:var:: ControlInput control_input (repeated)
	//@@
	//@@     The model input(s) that the server should use to communicate
	//@@     sequence start, stop, ready and similar control values to the
	//@@     model.
	//@@
	ControlInput         []*ModelSequenceBatching_ControlInput `protobuf:"bytes,2,rep,name=control_input,json=controlInput,proto3" json:"control_input,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                              `json:"-"`
	XXX_unrecognized     []byte                                `json:"-"`
	XXX_sizecache        int32                                 `json:"-"`
}

func (m *ModelSequenceBatching) Reset()         { *m = ModelSequenceBatching{} }
func (m *ModelSequenceBatching) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching) ProtoMessage()    {}
func (*ModelSequenceBatching) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{7}
}

func (m *ModelSequenceBatching) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching.Unmarshal(m, b)
}
func (m *ModelSequenceBatching) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching.Merge(m, src)
}
func (m *ModelSequenceBatching) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching.Size(m)
}
func (m *ModelSequenceBatching) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching proto.InternalMessageInfo

func (m *ModelSequenceBatching) GetMaxSequenceIdleMicroseconds() uint64 {
	if m != nil {
		return m.MaxSequenceIdleMicroseconds
	}
	return 0
}

func (m *ModelSequenceBatching) GetControlInput() []*ModelSequenceBatching_ControlInput {
	if m != nil {
		return m.ControlInput
	}
	return nil
}

//@@  .. cpp:var:: message Control
//@@
//@@     A control is a binary signal to a backend.
//@@
type ModelSequenceBatching_Control struct {
	//@@    .. cpp:var:: Kind kind
	//@@
	//@@       The kind of this control.
	//@@
	Kind ModelSequenceBatching_Control_Kind `protobuf:"varint,1,opt,name=kind,proto3,enum=nvidia.inferenceserver.ModelSequenceBatching_Control_Kind" json:"kind,omitempty"`
	//@@    .. cpp:var:: int32 int32_false_true (repeated)
	//@@
	//@@       The control's true and false setting is indicated by setting
	//@@       a value in an int32 tensor. The tensor must be a
	//@@       1-dimensional tensor with size equal to the batch size of
	//@@       the request. 'int32_false_true' must have two entries: the
	//@@       first the false value and the second the true value.
	//@@
	Int32FalseTrue []int32 `protobuf:"varint,2,rep,packed,name=int32_false_true,json=int32FalseTrue,proto3" json:"int32_false_true,omitempty"`
	//@@    .. cpp:var:: float fp32_false_true (repeated)
	//@@
	//@@       The control's true and false setting is indicated by setting
	//@@       a value in a fp32 tensor. The tensor must be a
	//@@       1-dimensional tensor with size equal to the batch size of
	//@@       the request. 'fp32_false_true' must have two entries: the
	//@@       first the false value and the second the true value.
	//@@
	Fp32FalseTrue        []float32 `protobuf:"fixed32,3,rep,packed,name=fp32_false_true,json=fp32FalseTrue,proto3" json:"fp32_false_true,omitempty"`
	XXX_NoUnkeyedLiteral struct{}  `json:"-"`
	XXX_unrecognized     []byte    `json:"-"`
	XXX_sizecache        int32     `json:"-"`
}

func (m *ModelSequenceBatching_Control) Reset()         { *m = ModelSequenceBatching_Control{} }
func (m *ModelSequenceBatching_Control) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching_Control) ProtoMessage()    {}
func (*ModelSequenceBatching_Control) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{7, 0}
}

func (m *ModelSequenceBatching_Control) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching_Control.Unmarshal(m, b)
}
func (m *ModelSequenceBatching_Control) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching_Control.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching_Control) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching_Control.Merge(m, src)
}
func (m *ModelSequenceBatching_Control) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching_Control.Size(m)
}
func (m *ModelSequenceBatching_Control) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching_Control.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching_Control proto.InternalMessageInfo

func (m *ModelSequenceBatching_Control) GetKind() ModelSequenceBatching_Control_Kind {
	if m != nil {
		return m.Kind
	}
	return ModelSequenceBatching_Control_CONTROL_SEQUENCE_START
}

func (m *ModelSequenceBatching_Control) GetInt32FalseTrue() []int32 {
	if m != nil {
		return m.Int32FalseTrue
	}
	return nil
}

func (m *ModelSequenceBatching_Control) GetFp32FalseTrue() []float32 {
	if m != nil {
		return m.Fp32FalseTrue
	}
	return nil
}

//@@  .. cpp:var:: message ControlInput
//@@
//@@     The sequence control values to communicate by a model input.
//@@
type ModelSequenceBatching_ControlInput struct {
	//@@    .. cpp:var:: string name
	//@@
	//@@       The name of the model input.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: Control control (repeated)
	//@@
	//@@       The control value(s) that should be communicated to the
	//@@       model using this model input.
	//@@
	Control              []*ModelSequenceBatching_Control `protobuf:"bytes,2,rep,name=control,proto3" json:"control,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                         `json:"-"`
	XXX_unrecognized     []byte                           `json:"-"`
	XXX_sizecache        int32                            `json:"-"`
}

func (m *ModelSequenceBatching_ControlInput) Reset()         { *m = ModelSequenceBatching_ControlInput{} }
func (m *ModelSequenceBatching_ControlInput) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching_ControlInput) ProtoMessage()    {}
func (*ModelSequenceBatching_ControlInput) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{7, 1}
}

func (m *ModelSequenceBatching_ControlInput) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching_ControlInput.Unmarshal(m, b)
}
func (m *ModelSequenceBatching_ControlInput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching_ControlInput.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching_ControlInput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching_ControlInput.Merge(m, src)
}
func (m *ModelSequenceBatching_ControlInput) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching_ControlInput.Size(m)
}
func (m *ModelSequenceBatching_ControlInput) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching_ControlInput.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching_ControlInput proto.InternalMessageInfo

func (m *ModelSequenceBatching_ControlInput) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelSequenceBatching_ControlInput) GetControl() []*ModelSequenceBatching_Control {
	if m != nil {
		return m.Control
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelEnsembling
//@@
//@@   Model ensembling configuration. These settings specify the models that
//@@   compose the ensemble and how data flows between the models.
//@@
type ModelEnsembling struct {
	//@@  .. cpp:var:: Step step (repeated)
	//@@
	//@@     The models and the input / output mappings used within the ensemble.
	//@@
	Step                 []*ModelEnsembling_Step `protobuf:"bytes,1,rep,name=step,proto3" json:"step,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                `json:"-"`
	XXX_unrecognized     []byte                  `json:"-"`
	XXX_sizecache        int32                   `json:"-"`
}

func (m *ModelEnsembling) Reset()         { *m = ModelEnsembling{} }
func (m *ModelEnsembling) String() string { return proto.CompactTextString(m) }
func (*ModelEnsembling) ProtoMessage()    {}
func (*ModelEnsembling) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8}
}

func (m *ModelEnsembling) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelEnsembling.Unmarshal(m, b)
}
func (m *ModelEnsembling) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelEnsembling.Marshal(b, m, deterministic)
}
func (m *ModelEnsembling) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelEnsembling.Merge(m, src)
}
func (m *ModelEnsembling) XXX_Size() int {
	return xxx_messageInfo_ModelEnsembling.Size(m)
}
func (m *ModelEnsembling) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelEnsembling.DiscardUnknown(m)
}

var xxx_messageInfo_ModelEnsembling proto.InternalMessageInfo

func (m *ModelEnsembling) GetStep() []*ModelEnsembling_Step {
	if m != nil {
		return m.Step
	}
	return nil
}

//@@  .. cpp:var:: message Step
//@@
//@@     Each step specifies a model included in the ensemble,
//@@     maps ensemble tensor names to the model input tensors,
//@@     and maps model output tensors to ensemble tensor names
//@@
type ModelEnsembling_Step struct {
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The name of the model to execute for this step of the ensemble.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@  .. cpp:var:: int64 model_version
	//@@
	//@@     The version of the model to use for inference. If -1
	//@@     the latest/most-recent version of the model is used.
	//@@
	ModelVersion int64 `protobuf:"varint,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	//@@  .. cpp:var:: map<string,string> input_map
	//@@
	//@@     Map from name of an input tensor on this step's model to ensemble
	//@@     tensor name. The ensemble tensor must have the same data type and
	//@@     shape as the model input. Each model input must be assigned to
	//@@     one ensemble tensor, but the same ensemble tensor can be assigned
	//@@     to multiple model inputs.
	//@@
	InputMap map[string]string `protobuf:"bytes,3,rep,name=input_map,json=inputMap,proto3" json:"input_map,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: map<string,string> output_map
	//@@
	//@@     Map from name of an output tensor on this step's model to ensemble
	//@@     tensor name. The data type and shape of the ensemble tensor will
	//@@     be inferred from the model output. It is optional to assign all
	//@@     model outputs to ensemble tensors. One ensemble tensor name
	//@@     can appear in an output map only once.
	//@@
	OutputMap            map[string]string `protobuf:"bytes,4,rep,name=output_map,json=outputMap,proto3" json:"output_map,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
	XXX_unrecognized     []byte            `json:"-"`
	XXX_sizecache        int32             `json:"-"`
}

func (m *ModelEnsembling_Step) Reset()         { *m = ModelEnsembling_Step{} }
func (m *ModelEnsembling_Step) String() string { return proto.CompactTextString(m) }
func (*ModelEnsembling_Step) ProtoMessage()    {}
func (*ModelEnsembling_Step) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8, 0}
}

func (m *ModelEnsembling_Step) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelEnsembling_Step.Unmarshal(m, b)
}
func (m *ModelEnsembling_Step) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelEnsembling_Step.Marshal(b, m, deterministic)
}
func (m *ModelEnsembling_Step) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelEnsembling_Step.Merge(m, src)
}
func (m *ModelEnsembling_Step) XXX_Size() int {
	return xxx_messageInfo_ModelEnsembling_Step.Size(m)
}
func (m *ModelEnsembling_Step) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelEnsembling_Step.DiscardUnknown(m)
}

var xxx_messageInfo_ModelEnsembling_Step proto.InternalMessageInfo

func (m *ModelEnsembling_Step) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelEnsembling_Step) GetModelVersion() int64 {
	if m != nil {
		return m.ModelVersion
	}
	return 0
}

func (m *ModelEnsembling_Step) GetInputMap() map[string]string {
	if m != nil {
		return m.InputMap
	}
	return nil
}

func (m *ModelEnsembling_Step) GetOutputMap() map[string]string {
	if m != nil {
		return m.OutputMap
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelParameter
//@@
//@@   A model parameter.
//@@
type ModelParameter struct {
	//@@  .. cpp:var:: string string_value
	//@@
	//@@     The string value of the parameter.
	//@@
	StringValue          string   `protobuf:"bytes,1,opt,name=string_value,json=stringValue,proto3" json:"string_value,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelParameter) Reset()         { *m = ModelParameter{} }
func (m *ModelParameter) String() string { return proto.CompactTextString(m) }
func (*ModelParameter) ProtoMessage()    {}
func (*ModelParameter) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{9}
}

func (m *ModelParameter) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelParameter.Unmarshal(m, b)
}
func (m *ModelParameter) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelParameter.Marshal(b, m, deterministic)
}
func (m *ModelParameter) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelParameter.Merge(m, src)
}
func (m *ModelParameter) XXX_Size() int {
	return xxx_messageInfo_ModelParameter.Size(m)
}
func (m *ModelParameter) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelParameter.DiscardUnknown(m)
}

var xxx_messageInfo_ModelParameter proto.InternalMessageInfo

func (m *ModelParameter) GetStringValue() string {
	if m != nil {
		return m.StringValue
	}
	return ""
}

//@@
//@@.. cpp:var:: message ModelConfig
//@@
//@@   A model configuration.
//@@
type ModelConfig struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string platform
	//@@
	//@@     The framework for the model. Possible values are
	//@@     "tensorrt_plan", "tensorflow_graphdef",
	//@@     "tensorflow_savedmodel", "caffe2_netdef",
	//@@     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
	//@@
	Platform string `protobuf:"bytes,2,opt,name=platform,proto3" json:"platform,omitempty"`
	//@@  .. cpp:var:: ModelVersionPolicy version_policy
	//@@
	//@@     Policy indicating which version(s) of the model will be served.
	//@@
	VersionPolicy *ModelVersionPolicy `protobuf:"bytes,3,opt,name=version_policy,json=versionPolicy,proto3" json:"version_policy,omitempty"`
	//@@  .. cpp:var:: int32 max_batch_size
	//@@
	//@@     Maximum batch size allowed for inference. This can only decrease
	//@@     what is allowed by the model itself. A max_batch_size value of 0
	//@@     indicates that batching is not allowed for the model and the
	//@@     dimension/shape of the input and output tensors must exactly
	//@@     match what is specified in the input and output configuration. A
	//@@     max_batch_size value > 0 indicates that batching is allowed and
	//@@     so the model expects the input tensors to have an additional
	//@@     initial dimension for the batching that is not specified in the
	//@@     input (for example, if the model supports batched inputs of
	//@@     2-dimensional tensors then the model configuration will specify
	//@@     the input shape as [ X, Y ] but the model will expect the actual
	//@@     input tensors to have shape [ N, X, Y ]). For max_batch_size > 0
	//@@     returned outputs will also have an additional initial dimension
	//@@     for the batch.
	//@@
	MaxBatchSize int32 `protobuf:"varint,4,opt,name=max_batch_size,json=maxBatchSize,proto3" json:"max_batch_size,omitempty"`
	//@@  .. cpp:var:: ModelInput input (repeated)
	//@@
	//@@     The inputs request by the model.
	//@@
	Input []*ModelInput `protobuf:"bytes,5,rep,name=input,proto3" json:"input,omitempty"`
	//@@  .. cpp:var:: ModelOutput output (repeated)
	//@@
	//@@     The outputs produced by the model.
	//@@
	Output []*ModelOutput `protobuf:"bytes,6,rep,name=output,proto3" json:"output,omitempty"`
	//@@  .. cpp:var:: ModelOptimizationPolicy optimization
	//@@
	//@@     Optimization configuration for the model. If not specified
	//@@     then default optimization policy is used.
	//@@
	Optimization *ModelOptimizationPolicy `protobuf:"bytes,12,opt,name=optimization,proto3" json:"optimization,omitempty"`
	//@@  .. cpp:var:: oneof scheduling_choice
	//@@
	//@@     The scheduling policy for the model. If not specified the
	//@@     default scheduling policy is used for the model. The default
	//@@     policy is to execute each inference request independently.
	//@@
	//
	// Types that are valid to be assigned to SchedulingChoice:
	//	*ModelConfig_DynamicBatching
	//	*ModelConfig_SequenceBatching
	//	*ModelConfig_EnsembleScheduling
	SchedulingChoice isModelConfig_SchedulingChoice `protobuf_oneof:"scheduling_choice"`
	//@@  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
	//@@
	//@@     Instances of this model. If not specified, one instance
	//@@     of the model will be instantiated on each available GPU.
	//@@
	InstanceGroup []*ModelInstanceGroup `protobuf:"bytes,7,rep,name=instance_group,json=instanceGroup,proto3" json:"instance_group,omitempty"`
	//@@  .. cpp:var:: string default_model_filename
	//@@
	//@@     Optional filename of the model file to use if a
	//@@     compute-capability specific model is not specified in
	//@@     :cpp:var:`cc_model_names`. If not specified the default name
	//@@     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
	//@@     'model.netdef' depending on the model type.
	//@@
	DefaultModelFilename string `protobuf:"bytes,8,opt,name=default_model_filename,json=defaultModelFilename,proto3" json:"default_model_filename,omitempty"`
	//@@  .. cpp:var:: map<string,string> cc_model_filenames
	//@@
	//@@     Optional map from CUDA compute capability to the filename of
	//@@     the model that supports that compute capability. The filename
	//@@     refers to a file within the model version directory.
	//@@
	CcModelFilenames map[string]string `protobuf:"bytes,9,rep,name=cc_model_filenames,json=ccModelFilenames,proto3" json:"cc_model_filenames,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: map<string,string> metric_tags
	//@@
	//@@     Optional metric tags. User-specific key-value pairs for metrics
	//@@     reported for this model. These tags are applied to the metrics
	//@@     reported on the HTTP metrics port.
	//@@
	MetricTags map[string]string `protobuf:"bytes,10,rep,name=metric_tags,json=metricTags,proto3" json:"metric_tags,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: map<string,ModelParameter> parameters
	//@@
	//@@     Optional model parameters. User-specified parameter values that
	//@@     are made available to custom backends.
	//@@
	Parameters           map[string]*ModelParameter `protobuf:"bytes,14,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                   `json:"-"`
	XXX_unrecognized     []byte                     `json:"-"`
	XXX_sizecache        int32                      `json:"-"`
}

func (m *ModelConfig) Reset()         { *m = ModelConfig{} }
func (m *ModelConfig) String() string { return proto.CompactTextString(m) }
func (*ModelConfig) ProtoMessage()    {}
func (*ModelConfig) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{10}
}

func (m *ModelConfig) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelConfig.Unmarshal(m, b)
}
func (m *ModelConfig) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelConfig.Marshal(b, m, deterministic)
}
func (m *ModelConfig) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelConfig.Merge(m, src)
}
func (m *ModelConfig) XXX_Size() int {
	return xxx_messageInfo_ModelConfig.Size(m)
}
func (m *ModelConfig) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelConfig.DiscardUnknown(m)
}

var xxx_messageInfo_ModelConfig proto.InternalMessageInfo

func (m *ModelConfig) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelConfig) GetPlatform() string {
	if m != nil {
		return m.Platform
	}
	return ""
}

func (m *ModelConfig) GetVersionPolicy() *ModelVersionPolicy {
	if m != nil {
		return m.VersionPolicy
	}
	return nil
}

func (m *ModelConfig) GetMaxBatchSize() int32 {
	if m != nil {
		return m.MaxBatchSize
	}
	return 0
}

func (m *ModelConfig) GetInput() []*ModelInput {
	if m != nil {
		return m.Input
	}
	return nil
}

func (m *ModelConfig) GetOutput() []*ModelOutput {
	if m != nil {
		return m.Output
	}
	return nil
}

func (m *ModelConfig) GetOptimization() *ModelOptimizationPolicy {
	if m != nil {
		return m.Optimization
	}
	return nil
}

type isModelConfig_SchedulingChoice interface {
	isModelConfig_SchedulingChoice()
}

type ModelConfig_DynamicBatching struct {
	DynamicBatching *ModelDynamicBatching `protobuf:"bytes,11,opt,name=dynamic_batching,json=dynamicBatching,proto3,oneof"`
}

type ModelConfig_SequenceBatching struct {
	SequenceBatching *ModelSequenceBatching `protobuf:"bytes,13,opt,name=sequence_batching,json=sequenceBatching,proto3,oneof"`
}

type ModelConfig_EnsembleScheduling struct {
	EnsembleScheduling *ModelEnsembling `protobuf:"bytes,15,opt,name=ensemble_scheduling,json=ensembleScheduling,proto3,oneof"`
}

func (*ModelConfig_DynamicBatching) isModelConfig_SchedulingChoice() {}

func (*ModelConfig_SequenceBatching) isModelConfig_SchedulingChoice() {}

func (*ModelConfig_EnsembleScheduling) isModelConfig_SchedulingChoice() {}

func (m *ModelConfig) GetSchedulingChoice() isModelConfig_SchedulingChoice {
	if m != nil {
		return m.SchedulingChoice
	}
	return nil
}

func (m *ModelConfig) GetDynamicBatching() *ModelDynamicBatching {
	if x, ok := m.GetSchedulingChoice().(*ModelConfig_DynamicBatching); ok {
		return x.DynamicBatching
	}
	return nil
}

func (m *ModelConfig) GetSequenceBatching() *ModelSequenceBatching {
	if x, ok := m.GetSchedulingChoice().(*ModelConfig_SequenceBatching); ok {
		return x.SequenceBatching
	}
	return nil
}

func (m *ModelConfig) GetEnsembleScheduling() *ModelEnsembling {
	if x, ok := m.GetSchedulingChoice().(*ModelConfig_EnsembleScheduling); ok {
		return x.EnsembleScheduling
	}
	return nil
}

func (m *ModelConfig) GetInstanceGroup() []*ModelInstanceGroup {
	if m != nil {
		return m.InstanceGroup
	}
	return nil
}

func (m *ModelConfig) GetDefaultModelFilename() string {
	if m != nil {
		return m.DefaultModelFilename
	}
	return ""
}

func (m *ModelConfig) GetCcModelFilenames() map[string]string {
	if m != nil {
		return m.CcModelFilenames
	}
	return nil
}

func (m *ModelConfig) GetMetricTags() map[string]string {
	if m != nil {
		return m.MetricTags
	}
	return nil
}

func (m *ModelConfig) GetParameters() map[string]*ModelParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*ModelConfig) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*ModelConfig_DynamicBatching)(nil),
		(*ModelConfig_SequenceBatching)(nil),
		(*ModelConfig_EnsembleScheduling)(nil),
	}
}

func init() {
	proto.RegisterEnum("nvidia.inferenceserver.DataType", DataType_name, DataType_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelInstanceGroup_Kind", ModelInstanceGroup_Kind_name, ModelInstanceGroup_Kind_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelInput_Format", ModelInput_Format_name, ModelInput_Format_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelOptimizationPolicy_ModelPriority", ModelOptimizationPolicy_ModelPriority_name, ModelOptimizationPolicy_ModelPriority_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelSequenceBatching_Control_Kind", ModelSequenceBatching_Control_Kind_name, ModelSequenceBatching_Control_Kind_value)
	proto.RegisterType((*ModelInstanceGroup)(nil), "nvidia.inferenceserver.ModelInstanceGroup")
	proto.RegisterType((*ModelTensorReshape)(nil), "nvidia.inferenceserver.ModelTensorReshape")
	proto.RegisterType((*ModelInput)(nil), "nvidia.inferenceserver.ModelInput")
	proto.RegisterType((*ModelOutput)(nil), "nvidia.inferenceserver.ModelOutput")
	proto.RegisterType((*ModelVersionPolicy)(nil), "nvidia.inferenceserver.ModelVersionPolicy")
	proto.RegisterType((*ModelVersionPolicy_Latest)(nil), "nvidia.inferenceserver.ModelVersionPolicy.Latest")
	proto.RegisterType((*ModelVersionPolicy_All)(nil), "nvidia.inferenceserver.ModelVersionPolicy.All")
	proto.RegisterType((*ModelVersionPolicy_Specific)(nil), "nvidia.inferenceserver.ModelVersionPolicy.Specific")
	proto.RegisterType((*ModelOptimizationPolicy)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy")
	proto.RegisterType((*ModelOptimizationPolicy_Graph)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.Graph")
	proto.RegisterType((*ModelOptimizationPolicy_Cuda)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.Cuda")
	proto.RegisterType((*ModelDynamicBatching)(nil), "nvidia.inferenceserver.ModelDynamicBatching")
	proto.RegisterType((*ModelSequenceBatching)(nil), "nvidia.inferenceserver.ModelSequenceBatching")
	proto.RegisterType((*ModelSequenceBatching_Control)(nil), "nvidia.inferenceserver.ModelSequenceBatching.Control")
	proto.RegisterType((*ModelSequenceBatching_ControlInput)(nil), "nvidia.inferenceserver.ModelSequenceBatching.ControlInput")
	proto.RegisterType((*ModelEnsembling)(nil), "nvidia.inferenceserver.ModelEnsembling")
	proto.RegisterType((*ModelEnsembling_Step)(nil), "nvidia.inferenceserver.ModelEnsembling.Step")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry")
	proto.RegisterType((*ModelParameter)(nil), "nvidia.inferenceserver.ModelParameter")
	proto.RegisterType((*ModelConfig)(nil), "nvidia.inferenceserver.ModelConfig")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelConfig.MetricTagsEntry")
	proto.RegisterMapType((map[string]*ModelParameter)(nil), "nvidia.inferenceserver.ModelConfig.ParametersEntry")
}

func init() { proto.RegisterFile("model_config.proto", fileDescriptor_5214c5af697e4203) }

var fileDescriptor_5214c5af697e4203 = []byte{
	// 1643 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x58, 0xdd, 0x72, 0x23, 0x47,
	0x15, 0xd6, 0xe8, 0xcf, 0xd2, 0xd1, 0x8f, 0x27, 0x1d, 0x67, 0x19, 0x04, 0x49, 0x19, 0x05, 0x16,
	0x63, 0x40, 0x61, 0x65, 0xc7, 0xb5, 0x64, 0xb3, 0x80, 0x2c, 0xc9, 0x2b, 0xd5, 0xda, 0x92, 0xb7,
	0x25, 0xef, 0xe2, 0x14, 0x55, 0x53, 0xbd, 0x33, 0x2d, 0x79, 0x2a, 0xf3, 0x97, 0xf9, 0x71, 0xad,
	0xf7, 0x0d, 0x78, 0x00, 0xee, 0x79, 0x08, 0x6e, 0x78, 0x0b, 0xae, 0xb9, 0xe3, 0x82, 0xab, 0xbc,
	0x01, 0x57, 0x54, 0x9f, 0x19, 0x8d, 0x47, 0x8a, 0xd1, 0x5a, 0x5b, 0xe4, 0xae, 0xcf, 0x51, 0x7f,
	0xdf, 0xf9, 0xe9, 0x33, 0xe7, 0x74, 0x0b, 0x88, 0xe5, 0xe8, 0xdc, 0x54, 0x35, 0xc7, 0x9e, 0x19,
	0xf3, 0x96, 0xeb, 0x39, 0x81, 0x43, 0x1e, 0xd8, 0xd7, 0x86, 0x6e, 0xb0, 0x96, 0x61, 0xcf, 0xb8,
	0xc7, 0x6d, 0x8d, 0xfb, 0xdc, 0xbb, 0xe6, 0x5e, 0xf3, 0x9f, 0x12, 0x90, 0x33, 0xb1, 0x7d, 0x68,
	0xfb, 0x01, 0xb3, 0x35, 0xfe, 0xcc, 0x73, 0x42, 0x97, 0x10, 0xc8, 0xdb, 0xcc, 0xe2, 0x8a, 0xb4,
	0x2b, 0xed, 0x95, 0x29, 0xae, 0x49, 0x17, 0xf2, 0x5f, 0x1b, 0xb6, 0xae, 0xe4, 0x77, 0xa5, 0xbd,
	0x7a, 0xfb, 0xb3, 0xd6, 0xdd, 0x8c, 0xad, 0xef, 0xb2, 0xb5, 0x9e, 0x1b, 0xb6, 0x4e, 0x11, 0x4c,
	0x76, 0xa0, 0xa0, 0x39, 0xa1, 0x1d, 0x28, 0xd9, 0x5d, 0x69, 0xaf, 0x40, 0x23, 0x41, 0x98, 0x9b,
	0xbb, 0xa1, 0xaf, 0xe4, 0x76, 0x73, 0x7b, 0x05, 0x8a, 0xeb, 0x66, 0x07, 0xf2, 0x02, 0x47, 0x6a,
	0x50, 0x7e, 0x3e, 0x1c, 0xf5, 0xd4, 0xce, 0xc5, 0x74, 0x2c, 0x67, 0x48, 0x15, 0x4a, 0x28, 0x3e,
	0x3b, 0xbf, 0x90, 0xa5, 0x44, 0xea, 0x9e, 0x5f, 0xc8, 0x59, 0x52, 0x07, 0x40, 0xe9, 0x6c, 0xdc,
	0xeb, 0x9f, 0xca, 0xb9, 0xe6, 0x7e, 0x1c, 0xdb, 0x94, 0xdb, 0xbe, 0xe3, 0x51, 0xee, 0x5f, 0x31,
	0x97, 0x0b, 0x17, 0x70, 0xa1, 0x48, 0xbb, 0xb9, 0xbd, 0x1c, 0x8d, 0x84, 0xe6, 0xdf, 0xb3, 0x00,
	0xb1, 0xeb, 0x6e, 0x18, 0xdc, 0x99, 0x80, 0xa7, 0x50, 0xd6, 0x59, 0xc0, 0xd4, 0xe0, 0xc6, 0xe5,
	0xe8, 0x7f, 0xbd, 0xbd, 0xfb, 0xbf, 0xb2, 0xd0, 0x63, 0x01, 0x9b, 0xde, 0xb8, 0x9c, 0x96, 0xf4,
	0x78, 0x45, 0x3a, 0x50, 0x9c, 0x39, 0x9e, 0xc5, 0x02, 0x25, 0x87, 0xd8, 0x5f, 0xbc, 0x23, 0x83,
	0x6e, 0x18, 0xb4, 0x4e, 0x10, 0x40, 0x63, 0xa0, 0xf0, 0x4a, 0x37, 0x2c, 0x5f, 0xc9, 0xa3, 0xe7,
	0xb8, 0x26, 0x3d, 0xd8, 0xf2, 0xa2, 0xc8, 0x94, 0xc2, 0xae, 0xb4, 0x57, 0x69, 0xef, 0xaf, 0xe5,
	0x5d, 0xca, 0x05, 0x5d, 0x40, 0x9b, 0x4f, 0xa0, 0x18, 0xd9, 0x22, 0xdb, 0x50, 0x39, 0x19, 0xd3,
	0xb3, 0xce, 0x54, 0x1d, 0x8d, 0x47, 0x7d, 0x39, 0x93, 0x56, 0x0c, 0x5e, 0x75, 0x65, 0x29, 0xad,
	0xe8, 0x0e, 0x5e, 0xc9, 0xd9, 0xe6, 0xbf, 0x24, 0xa8, 0x20, 0xf9, 0x38, 0x0c, 0xbe, 0xa7, 0xe4,
	0x2d, 0x22, 0xcf, 0xfd, 0xbf, 0x23, 0x27, 0x3f, 0x83, 0xba, 0xc9, 0x5e, 0x73, 0x53, 0x9d, 0x19,
	0x26, 0x47, 0xb7, 0xf3, 0xe8, 0x76, 0x0d, 0xb5, 0x27, 0xb1, 0xb2, 0xf9, 0xef, 0x6c, 0x5c, 0x4c,
	0x2f, 0xb9, 0xe7, 0x1b, 0x8e, 0x7d, 0xee, 0x98, 0x86, 0x76, 0x43, 0x9e, 0x43, 0xd1, 0x64, 0x01,
	0xf7, 0x03, 0x0c, 0xb6, 0xd2, 0x7e, 0xb4, 0xd6, 0x85, 0x25, 0x6c, 0xeb, 0x14, 0x81, 0x83, 0x0c,
	0x8d, 0x29, 0xc8, 0x31, 0xe4, 0x98, 0x69, 0x62, 0x76, 0x2a, 0xed, 0xd6, 0x06, 0x4c, 0x1d, 0xd3,
	0x1c, 0x64, 0xa8, 0x00, 0x93, 0x17, 0x50, 0xf2, 0x5d, 0xae, 0x19, 0x33, 0x43, 0xc3, 0x3a, 0xab,
	0xb4, 0x0f, 0x36, 0x20, 0x9a, 0xc4, 0xd0, 0x41, 0x86, 0x26, 0x34, 0x8d, 0x5f, 0x42, 0x31, 0x72,
	0x95, 0xfc, 0x04, 0xaa, 0x76, 0x68, 0xa9, 0xd7, 0x11, 0xc6, 0xc7, 0x98, 0x6b, 0xb4, 0x62, 0x87,
	0x56, 0x4c, 0xe3, 0x37, 0x0a, 0x90, 0xeb, 0x98, 0x66, 0xe3, 0x21, 0x94, 0x16, 0x5c, 0xa4, 0x01,
	0xa5, 0x14, 0x42, 0x9c, 0x5f, 0x22, 0x1f, 0x6f, 0x43, 0xcd, 0x45, 0xd3, 0xaa, 0x76, 0xe5, 0x18,
	0x1a, 0x6f, 0xfe, 0x25, 0x07, 0x3f, 0x88, 0x6a, 0xc9, 0x0d, 0x0c, 0xcb, 0x78, 0xcb, 0x82, 0x74,
	0xb2, 0x0b, 0x73, 0x8f, 0xb9, 0x57, 0x71, 0xae, 0x3f, 0x5f, 0x1b, 0xd8, 0x77, 0xf1, 0xad, 0x67,
	0x02, 0x4c, 0x23, 0x0e, 0x72, 0x09, 0x25, 0xd7, 0x33, 0x1c, 0xcf, 0x08, 0x6e, 0xe2, 0x7a, 0x7c,
	0xba, 0x29, 0x1f, 0xea, 0xcf, 0x63, 0x12, 0x9a, 0xd0, 0x91, 0x01, 0xe4, 0xb5, 0x50, 0x67, 0x71,
	0xfe, 0x0f, 0x37, 0xa5, 0xed, 0x86, 0x3a, 0xa3, 0xc8, 0xd0, 0xf8, 0x18, 0x0a, 0xe8, 0xb4, 0x68,
	0x5a, 0x26, 0xbf, 0xe6, 0x26, 0x86, 0x5e, 0xa0, 0x91, 0xd0, 0xf8, 0x04, 0xf2, 0x62, 0x33, 0x79,
	0x00, 0x45, 0x0c, 0x2a, 0x3a, 0x91, 0x12, 0x8d, 0xa5, 0xe6, 0x10, 0x6a, 0x4b, 0x3e, 0x92, 0x1d,
	0x90, 0xcf, 0xe9, 0x70, 0x4c, 0x87, 0xd3, 0x4b, 0xb5, 0xd7, 0x3f, 0xe9, 0x5c, 0x9c, 0x4e, 0xe5,
	0x0c, 0x91, 0xa1, 0x9a, 0x68, 0xcf, 0x3a, 0x7f, 0x94, 0xa5, 0x65, 0xcd, 0x70, 0x24, 0x67, 0x9b,
	0x7f, 0x96, 0x60, 0x07, 0xb9, 0x7a, 0x37, 0x36, 0xb3, 0x0c, 0xed, 0x98, 0x05, 0xda, 0x95, 0x61,
	0xcf, 0xc9, 0x6f, 0x60, 0xc7, 0xf5, 0xf8, 0x8c, 0x7b, 0x1e, 0xd7, 0xd5, 0xd7, 0x42, 0xab, 0xfa,
	0xc6, 0xdb, 0xa8, 0xbb, 0x16, 0x28, 0x49, 0x7e, 0x43, 0xc0, 0xc4, 0x78, 0xcb, 0xc9, 0xef, 0xe1,
	0xc7, 0x16, 0x7b, 0xa3, 0x7e, 0x13, 0xf2, 0x90, 0xab, 0x3a, 0x37, 0xd9, 0x8d, 0x6a, 0x19, 0x9a,
	0xe7, 0xf8, 0x5c, 0x73, 0x6c, 0xdd, 0xc7, 0xd3, 0xc8, 0xd3, 0x1f, 0x5a, 0xec, 0xcd, 0x0b, 0xb1,
	0xa5, 0x27, 0x76, 0x9c, 0xa5, 0x36, 0x34, 0xff, 0x9a, 0x87, 0x8f, 0xd0, 0x97, 0x09, 0xff, 0x26,
	0x14, 0x09, 0x4d, 0x9c, 0xe9, 0xc2, 0x27, 0x82, 0xda, 0x8f, 0xf5, 0xaa, 0xa1, 0x9b, 0x7c, 0x99,
	0x5c, 0x42, 0xf2, 0x1f, 0x59, 0xec, 0xcd, 0x02, 0x3c, 0xd4, 0x4d, 0x9e, 0xa6, 0x27, 0x2a, 0xd4,
	0x34, 0xc7, 0x0e, 0x3c, 0xc7, 0x54, 0x0d, 0xd1, 0x85, 0x95, 0xec, 0x6e, 0x6e, 0xaf, 0xd2, 0xfe,
	0x62, 0xed, 0x39, 0xae, 0xba, 0xd2, 0xea, 0x46, 0x14, 0xd8, 0xc7, 0x69, 0x55, 0x4b, 0x49, 0x8d,
	0x6f, 0x25, 0xd8, 0x8a, 0x7f, 0x26, 0xa3, 0x78, 0xaa, 0x4a, 0x58, 0x82, 0xef, 0x67, 0x23, 0x3d,
	0x60, 0xf7, 0x40, 0x36, 0xec, 0xe0, 0xa0, 0xad, 0xce, 0x98, 0xe9, 0x73, 0x35, 0xf0, 0x42, 0x8e,
	0xfe, 0x17, 0x68, 0x1d, 0xf5, 0x27, 0x42, 0x3d, 0xf5, 0x42, 0x4e, 0x1e, 0xc2, 0xf6, 0xcc, 0x5d,
	0xde, 0x28, 0xba, 0x6b, 0x96, 0xd6, 0x84, 0x3a, 0xd9, 0xd7, 0xfc, 0x5d, 0x3c, 0x88, 0x1b, 0xf0,
	0xa0, 0x3b, 0x1e, 0x4d, 0xe9, 0xf8, 0x54, 0x9d, 0xf4, 0x5f, 0x5c, 0xf4, 0x47, 0xdd, 0xbe, 0x3a,
	0x99, 0x76, 0xa8, 0xa8, 0xa0, 0xbb, 0x7e, 0xa3, 0xfd, 0x4e, 0xef, 0x52, 0x96, 0x1a, 0x3e, 0x54,
	0xd3, 0xb9, 0xb8, 0x73, 0x3a, 0x8c, 0x61, 0x2b, 0xce, 0x50, 0x9c, 0xec, 0xcf, 0xdf, 0x2b, 0x11,
	0x74, 0xc1, 0xd2, 0xfc, 0x47, 0x0e, 0xb6, 0x71, 0x6b, 0xdf, 0xf6, 0xb9, 0xf5, 0xda, 0x14, 0xc5,
	0xf1, 0x07, 0xc8, 0xfb, 0x01, 0x77, 0xb1, 0x32, 0x2b, 0xed, 0x5f, 0xad, 0xb5, 0x70, 0x0b, 0x6b,
	0x4d, 0x02, 0xee, 0x52, 0x44, 0x36, 0xfe, 0x93, 0x85, 0xbc, 0x10, 0xc9, 0xc7, 0x00, 0xd1, 0x25,
	0x2b, 0x15, 0x49, 0x19, 0x35, 0x23, 0x11, 0xce, 0xa7, 0x50, 0x8b, 0x7e, 0x8e, 0xfb, 0x1c, 0x96,
	0x74, 0x8e, 0x56, 0xad, 0x54, 0xc7, 0x25, 0xaf, 0xa0, 0x8c, 0xe5, 0xa5, 0x5a, 0xcc, 0xc5, 0xcc,
	0xbf, 0xab, 0xc4, 0x56, 0x7c, 0x6a, 0x61, 0x3a, 0xcf, 0x98, 0xdb, 0xb7, 0x03, 0xef, 0x86, 0x96,
	0x8c, 0x58, 0x24, 0x5f, 0x01, 0x38, 0x38, 0x88, 0x91, 0x39, 0x8f, 0xcc, 0x4f, 0x36, 0x62, 0x8e,
	0xe6, 0x78, 0x42, 0x5d, 0x76, 0x16, 0x72, 0xe3, 0x09, 0xd4, 0x96, 0xcc, 0x12, 0x19, 0x72, 0x5f,
	0xf3, 0x9b, 0x38, 0x05, 0x62, 0x29, 0x5a, 0xd5, 0x35, 0x33, 0xc3, 0x68, 0xca, 0x97, 0x69, 0x24,
	0x7c, 0x91, 0x7d, 0x2c, 0x35, 0xbe, 0x84, 0xfa, 0x32, 0xf3, 0x26, 0xe8, 0xe6, 0x01, 0xd4, 0xa3,
	0x66, 0xc6, 0x3c, 0x66, 0xf1, 0x80, 0x7b, 0x62, 0x1c, 0xf9, 0x81, 0x67, 0xd8, 0x73, 0x35, 0x82,
	0x44, 0x34, 0x95, 0x48, 0xf7, 0x52, 0xa8, 0x9a, 0x7f, 0x83, 0xf8, 0x6a, 0xd2, 0xc5, 0xdb, 0xf0,
	0x9d, 0xc5, 0xd7, 0x80, 0x92, 0x6b, 0xb2, 0x40, 0xdc, 0xb1, 0x62, 0xab, 0x89, 0x4c, 0x5e, 0x40,
	0x3d, 0x3e, 0x43, 0x35, 0x9a, 0x53, 0x71, 0x53, 0xdf, 0xbf, 0xff, 0x50, 0xa5, 0xb5, 0xeb, 0xa5,
	0x2b, 0xc3, 0x4f, 0xa1, 0x2e, 0x7a, 0x54, 0xaa, 0x55, 0xe6, 0xb1, 0xa7, 0x57, 0x2d, 0xf6, 0xe6,
	0xb6, 0x49, 0x3e, 0x86, 0x42, 0xd4, 0x7c, 0x0a, 0x78, 0x7e, 0xcd, 0x77, 0x5f, 0x16, 0x69, 0x04,
	0x20, 0x4f, 0xa0, 0x18, 0x9d, 0x97, 0x52, 0x44, 0xe8, 0xa7, 0xeb, 0xe7, 0x0f, 0x6e, 0xa5, 0x31,
	0x84, 0x4c, 0xa0, 0xea, 0xa4, 0x26, 0x92, 0x52, 0xc5, 0x68, 0x3f, 0xdb, 0x70, 0x84, 0xd1, 0x25,
	0x12, 0x72, 0x09, 0xb2, 0x1e, 0x4d, 0x8d, 0x28, 0x6a, 0xc3, 0x9e, 0x2b, 0x15, 0x24, 0x5e, 0xff,
	0x11, 0xae, 0x8c, 0x9a, 0x41, 0x86, 0x6e, 0xeb, 0x2b, 0xd3, 0xe7, 0x4f, 0xf0, 0x41, 0xd2, 0xec,
	0x13, 0xee, 0x1a, 0x72, 0xff, 0x7a, 0xa3, 0x16, 0x32, 0xc8, 0x50, 0xd9, 0x5f, 0x1d, 0x27, 0x5f,
	0xc1, 0x87, 0x3c, 0xfa, 0x34, 0xb8, 0xea, 0x6b, 0x57, 0x5c, 0x0f, 0xc5, 0x37, 0xa2, 0x6c, 0x23,
	0xff, 0xcf, 0xef, 0xf9, 0x49, 0x0d, 0x32, 0x94, 0x2c, 0x58, 0x26, 0x09, 0x89, 0xa8, 0x2c, 0x23,
	0x7e, 0x25, 0xa9, 0x73, 0xf1, 0x4c, 0x52, 0xb6, 0xf0, 0xb8, 0xf6, 0xef, 0xff, 0xb0, 0xa2, 0x35,
	0x63, 0xe9, 0xd5, 0x76, 0x08, 0x0f, 0x74, 0x3e, 0x63, 0xa1, 0x19, 0xa8, 0x51, 0xfb, 0x49, 0xae,
	0xb4, 0x25, 0x2c, 0xeb, 0x9d, 0xf8, 0x57, 0x64, 0x5a, 0xdc, 0x6c, 0xc9, 0x1c, 0x88, 0xa6, 0xad,
	0x00, 0x7c, 0xa5, 0x8c, 0xce, 0xfc, 0x76, 0xad, 0x33, 0xd1, 0x37, 0xd5, 0xea, 0x6a, 0x4b, 0x84,
	0x7e, 0xd4, 0x34, 0x64, 0x6d, 0x45, 0x4d, 0xa6, 0x50, 0xb1, 0x78, 0xe0, 0x19, 0x9a, 0x1a, 0xb0,
	0xb9, 0xaf, 0x00, 0x5a, 0x38, 0xb8, 0x8f, 0x85, 0x33, 0x84, 0x4d, 0xd9, 0x3c, 0xe6, 0x06, 0x2b,
	0x51, 0x90, 0x09, 0x80, 0xbb, 0xe8, 0x08, 0xbe, 0x52, 0xbf, 0x3f, 0x69, 0xd2, 0x47, 0x16, 0xa4,
	0xb7, 0x34, 0x8d, 0x2e, 0x7c, 0x74, 0x67, 0x54, 0x1b, 0xb5, 0xbb, 0xa7, 0xb0, 0xbd, 0xe2, 0xf8,
	0x46, 0x70, 0x0e, 0xdb, 0x2b, 0x2e, 0xde, 0x01, 0xff, 0x32, 0x0d, 0xaf, 0xb4, 0x1f, 0xae, 0x0d,
	0x3c, 0xa1, 0x4b, 0x99, 0x39, 0xfe, 0x10, 0x3e, 0xb8, 0x2d, 0xed, 0xf8, 0x16, 0xbe, 0xff, 0xad,
	0x04, 0xa5, 0xc5, 0x2b, 0x4c, 0x5c, 0x06, 0xa7, 0x97, 0xe7, 0x7d, 0x75, 0x38, 0x7a, 0xd9, 0x39,
	0x1d, 0xf6, 0xe4, 0x8c, 0x78, 0x93, 0xa3, 0xe6, 0x78, 0x3c, 0x3e, 0x95, 0x25, 0xf1, 0xee, 0x46,
	0xf1, 0x62, 0x38, 0x9a, 0x3e, 0x96, 0xb3, 0xe2, 0x81, 0x98, 0xc8, 0x8f, 0x8e, 0xe4, 0xdc, 0x92,
	0xe2, 0xa0, 0x2d, 0xe7, 0x97, 0x14, 0x47, 0x87, 0x72, 0x21, 0x61, 0x44, 0x86, 0x62, 0xc2, 0x18,
	0x11, 0x6c, 0xa5, 0xe5, 0x83, 0xb6, 0x5c, 0x4a, 0xcb, 0x47, 0x87, 0x72, 0x39, 0x81, 0x9f, 0x9c,
	0x3f, 0x3a, 0x92, 0x21, 0x25, 0x1e, 0xb4, 0xe5, 0x4a, 0x4a, 0x3c, 0x3a, 0x94, 0xab, 0x89, 0xf1,
	0xc9, 0x94, 0x0e, 0x47, 0xcf, 0xe4, 0xda, 0xeb, 0x22, 0xfe, 0x47, 0x72, 0xf0, 0xdf, 0x00, 0x00,
	0x00, 0xff, 0xff, 0xac, 0x03, 0x9b, 0x02, 0x39, 0x11, 0x00, 0x00,
}
