// Code generated by protoc-gen-go. DO NOT EDIT.
// source: grpc_service.proto

package nvidia_inferenceserver

import (
	context "context"
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

//@@  .. cpp:enum:: Type
//@@
//@@     Types of control operation
//@@
type ModelControlRequest_Type int32

const (
	//@@    .. cpp:enumerator:: Type::UNLOAD = 0
	//@@
	//@@       To unload the specified model.
	//@@
	ModelControlRequest_UNLOAD ModelControlRequest_Type = 0
	//@@    .. cpp:enumerator:: Type::LOAD = 1
	//@@
	//@@       To load the specified model. If the model has been loaded,
	//@@       it will be reloaded to fetch the latest change.
	//@@
	ModelControlRequest_LOAD ModelControlRequest_Type = 1
)

var ModelControlRequest_Type_name = map[int32]string{
	0: "UNLOAD",
	1: "LOAD",
}

var ModelControlRequest_Type_value = map[string]int32{
	"UNLOAD": 0,
	"LOAD":   1,
}

func (x ModelControlRequest_Type) String() string {
	return proto.EnumName(ModelControlRequest_Type_name, int32(x))
}

func (ModelControlRequest_Type) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{6, 0}
}

//@@  .. cpp:enum:: Type
//@@
//@@     Types of control operations for shared memory
//@@
type SharedMemoryControlRequest_Type int32

const (
	//@@    .. cpp:enumerator:: Type::REGISTER = 0
	//@@
	//@@       To register the specified shared memory region.
	//@@
	SharedMemoryControlRequest_REGISTER SharedMemoryControlRequest_Type = 0
	//@@    .. cpp:enumerator:: Type::UNREGISTER = 1
	//@@
	//@@       To unregister the specified shared memory region.
	//@@
	SharedMemoryControlRequest_UNREGISTER SharedMemoryControlRequest_Type = 1
	//@@    .. cpp:enumerator:: Type::UNREGISTER_ALL = 2
	//@@
	//@@       To unregister all active shared memory regions.
	//@@
	SharedMemoryControlRequest_UNREGISTER_ALL SharedMemoryControlRequest_Type = 2
)

var SharedMemoryControlRequest_Type_name = map[int32]string{
	0: "REGISTER",
	1: "UNREGISTER",
	2: "UNREGISTER_ALL",
}

var SharedMemoryControlRequest_Type_value = map[string]int32{
	"REGISTER":       0,
	"UNREGISTER":     1,
	"UNREGISTER_ALL": 2,
}

func (x SharedMemoryControlRequest_Type) String() string {
	return proto.EnumName(SharedMemoryControlRequest_Type_name, int32(x))
}

func (SharedMemoryControlRequest_Type) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{9, 0}
}

//@@
//@@.. cpp:var:: message StatusRequest
//@@
//@@   Request message for Status gRPC endpoint.
//@@
type StatusRequest struct {
	//@@
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The specific model status to be returned. If empty return status
	//@@     for all models.
	//@@
	ModelName            string   `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StatusRequest) Reset()         { *m = StatusRequest{} }
func (m *StatusRequest) String() string { return proto.CompactTextString(m) }
func (*StatusRequest) ProtoMessage()    {}
func (*StatusRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{0}
}

func (m *StatusRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StatusRequest.Unmarshal(m, b)
}
func (m *StatusRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StatusRequest.Marshal(b, m, deterministic)
}
func (m *StatusRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StatusRequest.Merge(m, src)
}
func (m *StatusRequest) XXX_Size() int {
	return xxx_messageInfo_StatusRequest.Size(m)
}
func (m *StatusRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_StatusRequest.DiscardUnknown(m)
}

var xxx_messageInfo_StatusRequest proto.InternalMessageInfo

func (m *StatusRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

//@@
//@@.. cpp:var:: message StatusResponse
//@@
//@@   Response message for Status gRPC endpoint.
//@@
type StatusResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	//@@
	//@@  .. cpp:var:: ServerStatus server_status
	//@@
	//@@     The server and model status.
	//@@
	ServerStatus         *ServerStatus `protobuf:"bytes,2,opt,name=server_status,json=serverStatus,proto3" json:"server_status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}      `json:"-"`
	XXX_unrecognized     []byte        `json:"-"`
	XXX_sizecache        int32         `json:"-"`
}

func (m *StatusResponse) Reset()         { *m = StatusResponse{} }
func (m *StatusResponse) String() string { return proto.CompactTextString(m) }
func (*StatusResponse) ProtoMessage()    {}
func (*StatusResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{1}
}

func (m *StatusResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StatusResponse.Unmarshal(m, b)
}
func (m *StatusResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StatusResponse.Marshal(b, m, deterministic)
}
func (m *StatusResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StatusResponse.Merge(m, src)
}
func (m *StatusResponse) XXX_Size() int {
	return xxx_messageInfo_StatusResponse.Size(m)
}
func (m *StatusResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_StatusResponse.DiscardUnknown(m)
}

var xxx_messageInfo_StatusResponse proto.InternalMessageInfo

func (m *StatusResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

func (m *StatusResponse) GetServerStatus() *ServerStatus {
	if m != nil {
		return m.ServerStatus
	}
	return nil
}

//@@
//@@.. cpp:var:: message ProfileRequest
//@@
//@@   Request message for Profile gRPC endpoint.
//@@
type ProfileRequest struct {
	//@@
	//@@  .. cpp:var:: string cmd
	//@@
	//@@     The requested profiling action: 'start' requests that GPU
	//@@     profiling be enabled on all GPUs controlled by the inference
	//@@     server; 'stop' requests that GPU profiling be disabled on all GPUs
	//@@     controlled by the inference server.
	//@@
	Cmd                  string   `protobuf:"bytes,1,opt,name=cmd,proto3" json:"cmd,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ProfileRequest) Reset()         { *m = ProfileRequest{} }
func (m *ProfileRequest) String() string { return proto.CompactTextString(m) }
func (*ProfileRequest) ProtoMessage()    {}
func (*ProfileRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{2}
}

func (m *ProfileRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProfileRequest.Unmarshal(m, b)
}
func (m *ProfileRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProfileRequest.Marshal(b, m, deterministic)
}
func (m *ProfileRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProfileRequest.Merge(m, src)
}
func (m *ProfileRequest) XXX_Size() int {
	return xxx_messageInfo_ProfileRequest.Size(m)
}
func (m *ProfileRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ProfileRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ProfileRequest proto.InternalMessageInfo

func (m *ProfileRequest) GetCmd() string {
	if m != nil {
		return m.Cmd
	}
	return ""
}

//@@
//@@.. cpp:var:: message ProfileResponse
//@@
//@@   Response message for Profile gRPC endpoint.
//@@
type ProfileResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus        *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}       `json:"-"`
	XXX_unrecognized     []byte         `json:"-"`
	XXX_sizecache        int32          `json:"-"`
}

func (m *ProfileResponse) Reset()         { *m = ProfileResponse{} }
func (m *ProfileResponse) String() string { return proto.CompactTextString(m) }
func (*ProfileResponse) ProtoMessage()    {}
func (*ProfileResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{3}
}

func (m *ProfileResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProfileResponse.Unmarshal(m, b)
}
func (m *ProfileResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProfileResponse.Marshal(b, m, deterministic)
}
func (m *ProfileResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProfileResponse.Merge(m, src)
}
func (m *ProfileResponse) XXX_Size() int {
	return xxx_messageInfo_ProfileResponse.Size(m)
}
func (m *ProfileResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ProfileResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ProfileResponse proto.InternalMessageInfo

func (m *ProfileResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

//@@
//@@.. cpp:var:: message HealthRequest
//@@
//@@   Request message for Health gRPC endpoint.
//@@
type HealthRequest struct {
	//@@
	//@@  .. cpp:var:: string mode
	//@@
	//@@     The requested health action: 'live' requests the liveness
	//@@     state of the inference server; 'ready' requests the readiness state
	//@@     of the inference server.
	//@@
	Mode                 string   `protobuf:"bytes,1,opt,name=mode,proto3" json:"mode,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *HealthRequest) Reset()         { *m = HealthRequest{} }
func (m *HealthRequest) String() string { return proto.CompactTextString(m) }
func (*HealthRequest) ProtoMessage()    {}
func (*HealthRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{4}
}

func (m *HealthRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_HealthRequest.Unmarshal(m, b)
}
func (m *HealthRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_HealthRequest.Marshal(b, m, deterministic)
}
func (m *HealthRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_HealthRequest.Merge(m, src)
}
func (m *HealthRequest) XXX_Size() int {
	return xxx_messageInfo_HealthRequest.Size(m)
}
func (m *HealthRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_HealthRequest.DiscardUnknown(m)
}

var xxx_messageInfo_HealthRequest proto.InternalMessageInfo

func (m *HealthRequest) GetMode() string {
	if m != nil {
		return m.Mode
	}
	return ""
}

//@@
//@@.. cpp:var:: message HealthResponse
//@@
//@@   Response message for Health gRPC endpoint.
//@@
type HealthResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	//@@
	//@@  .. cpp:var:: bool health
	//@@
	//@@     The result of the request. True indicates the inference server is
	//@@     live/ready, false indicates the inference server is not live/ready.
	//@@
	Health               bool     `protobuf:"varint,2,opt,name=health,proto3" json:"health,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *HealthResponse) Reset()         { *m = HealthResponse{} }
func (m *HealthResponse) String() string { return proto.CompactTextString(m) }
func (*HealthResponse) ProtoMessage()    {}
func (*HealthResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{5}
}

func (m *HealthResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_HealthResponse.Unmarshal(m, b)
}
func (m *HealthResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_HealthResponse.Marshal(b, m, deterministic)
}
func (m *HealthResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_HealthResponse.Merge(m, src)
}
func (m *HealthResponse) XXX_Size() int {
	return xxx_messageInfo_HealthResponse.Size(m)
}
func (m *HealthResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_HealthResponse.DiscardUnknown(m)
}

var xxx_messageInfo_HealthResponse proto.InternalMessageInfo

func (m *HealthResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

func (m *HealthResponse) GetHealth() bool {
	if m != nil {
		return m.Health
	}
	return false
}

//@@
//@@.. cpp:var:: message ModelControlRequest
//@@
//@@   Request message for ModelControl gRPC endpoint.
//@@
type ModelControlRequest struct {
	//@@
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The target model name.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@
	//@@  .. cpp:var:: Type type
	//@@
	//@@     The control type that is operated on the specified model.
	//@@
	Type                 ModelControlRequest_Type `protobuf:"varint,2,opt,name=type,proto3,enum=nvidia.inferenceserver.ModelControlRequest_Type" json:"type,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                 `json:"-"`
	XXX_unrecognized     []byte                   `json:"-"`
	XXX_sizecache        int32                    `json:"-"`
}

func (m *ModelControlRequest) Reset()         { *m = ModelControlRequest{} }
func (m *ModelControlRequest) String() string { return proto.CompactTextString(m) }
func (*ModelControlRequest) ProtoMessage()    {}
func (*ModelControlRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{6}
}

func (m *ModelControlRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelControlRequest.Unmarshal(m, b)
}
func (m *ModelControlRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelControlRequest.Marshal(b, m, deterministic)
}
func (m *ModelControlRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelControlRequest.Merge(m, src)
}
func (m *ModelControlRequest) XXX_Size() int {
	return xxx_messageInfo_ModelControlRequest.Size(m)
}
func (m *ModelControlRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelControlRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelControlRequest proto.InternalMessageInfo

func (m *ModelControlRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelControlRequest) GetType() ModelControlRequest_Type {
	if m != nil {
		return m.Type
	}
	return ModelControlRequest_UNLOAD
}

//@@
//@@.. cpp:var:: message ModelControlResponse
//@@
//@@   Response message for ModelControl gRPC endpoint.
//@@
type ModelControlResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus        *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}       `json:"-"`
	XXX_unrecognized     []byte         `json:"-"`
	XXX_sizecache        int32          `json:"-"`
}

func (m *ModelControlResponse) Reset()         { *m = ModelControlResponse{} }
func (m *ModelControlResponse) String() string { return proto.CompactTextString(m) }
func (*ModelControlResponse) ProtoMessage()    {}
func (*ModelControlResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{7}
}

func (m *ModelControlResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelControlResponse.Unmarshal(m, b)
}
func (m *ModelControlResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelControlResponse.Marshal(b, m, deterministic)
}
func (m *ModelControlResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelControlResponse.Merge(m, src)
}
func (m *ModelControlResponse) XXX_Size() int {
	return xxx_messageInfo_ModelControlResponse.Size(m)
}
func (m *ModelControlResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelControlResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelControlResponse proto.InternalMessageInfo

func (m *ModelControlResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

//@@.. cpp:var:: message InferSharedMemoryRegion
//@@
//@@   The meta-data for the shared memory region from which to read the input
//@@   data and/or write the output data.
//@@
type InferSharedMemoryRegion struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name for this shared memory region.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string shm_key
	//@@
	//@@     The name of the shared memory region that holds the input data
	//@@     (or where the output data should be written).
	//@@
	ShmKey string `protobuf:"bytes,2,opt,name=shm_key,json=shmKey,proto3" json:"shm_key,omitempty"`
	//@@  .. cpp:var:: uint64 offset
	//@@
	//@@     The offset from the start of the shared memory region.
	//@@     start = offset, end = offset + size;
	//@@
	Offset uint64 `protobuf:"varint,3,opt,name=offset,proto3" json:"offset,omitempty"`
	//@@  .. cpp:var:: uint64 byte_size
	//@@
	//@@     Size of the memory block, in bytes.
	//@@
	ByteSize             uint64   `protobuf:"varint,4,opt,name=byte_size,json=byteSize,proto3" json:"byte_size,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferSharedMemoryRegion) Reset()         { *m = InferSharedMemoryRegion{} }
func (m *InferSharedMemoryRegion) String() string { return proto.CompactTextString(m) }
func (*InferSharedMemoryRegion) ProtoMessage()    {}
func (*InferSharedMemoryRegion) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{8}
}

func (m *InferSharedMemoryRegion) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferSharedMemoryRegion.Unmarshal(m, b)
}
func (m *InferSharedMemoryRegion) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferSharedMemoryRegion.Marshal(b, m, deterministic)
}
func (m *InferSharedMemoryRegion) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferSharedMemoryRegion.Merge(m, src)
}
func (m *InferSharedMemoryRegion) XXX_Size() int {
	return xxx_messageInfo_InferSharedMemoryRegion.Size(m)
}
func (m *InferSharedMemoryRegion) XXX_DiscardUnknown() {
	xxx_messageInfo_InferSharedMemoryRegion.DiscardUnknown(m)
}

var xxx_messageInfo_InferSharedMemoryRegion proto.InternalMessageInfo

func (m *InferSharedMemoryRegion) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *InferSharedMemoryRegion) GetShmKey() string {
	if m != nil {
		return m.ShmKey
	}
	return ""
}

func (m *InferSharedMemoryRegion) GetOffset() uint64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

func (m *InferSharedMemoryRegion) GetByteSize() uint64 {
	if m != nil {
		return m.ByteSize
	}
	return 0
}

//@@
//@@.. cpp:var:: message SharedMemoryControlRequest
//@@
//@@   Request message for managing registered shared memory regions in TRTIS.
//@@
type SharedMemoryControlRequest struct {
	//@@
	//@@  .. cpp:var:: Type type
	//@@
	//@@     The control type that states whether to register/unregister the
	//@@     specified shared memory region. Unregister all active shared memory
	//@@     regions or get the list of active shared memory regions
	//@@
	Type SharedMemoryControlRequest_Type `protobuf:"varint,1,opt,name=type,proto3,enum=nvidia.inferenceserver.SharedMemoryControlRequest_Type" json:"type,omitempty"`
	//@@  .. cpp:var:: InferSharedMemoryRegion shared_memory_region
	//@@
	//@@     The shared memory region to register or unregister.
	//@@     All fields are needed to REGISTER the shared memory region.
	//@@     Only 'name' is needed to UNREGISTER the shared memory region.
	//@@     No fields are needed to UNREGISTER_ALL.
	//@@
	SharedMemoryRegion   *InferSharedMemoryRegion `protobuf:"bytes,2,opt,name=shared_memory_region,json=sharedMemoryRegion,proto3" json:"shared_memory_region,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                 `json:"-"`
	XXX_unrecognized     []byte                   `json:"-"`
	XXX_sizecache        int32                    `json:"-"`
}

func (m *SharedMemoryControlRequest) Reset()         { *m = SharedMemoryControlRequest{} }
func (m *SharedMemoryControlRequest) String() string { return proto.CompactTextString(m) }
func (*SharedMemoryControlRequest) ProtoMessage()    {}
func (*SharedMemoryControlRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{9}
}

func (m *SharedMemoryControlRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SharedMemoryControlRequest.Unmarshal(m, b)
}
func (m *SharedMemoryControlRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SharedMemoryControlRequest.Marshal(b, m, deterministic)
}
func (m *SharedMemoryControlRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SharedMemoryControlRequest.Merge(m, src)
}
func (m *SharedMemoryControlRequest) XXX_Size() int {
	return xxx_messageInfo_SharedMemoryControlRequest.Size(m)
}
func (m *SharedMemoryControlRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_SharedMemoryControlRequest.DiscardUnknown(m)
}

var xxx_messageInfo_SharedMemoryControlRequest proto.InternalMessageInfo

func (m *SharedMemoryControlRequest) GetType() SharedMemoryControlRequest_Type {
	if m != nil {
		return m.Type
	}
	return SharedMemoryControlRequest_REGISTER
}

func (m *SharedMemoryControlRequest) GetSharedMemoryRegion() *InferSharedMemoryRegion {
	if m != nil {
		return m.SharedMemoryRegion
	}
	return nil
}

//@@
//@@.. cpp:var:: message SharedMemoryControlResponse
//@@
//@@   Response message for SharedMemoryControl gRPC endpoint.
//@@
type SharedMemoryControlResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus        *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}       `json:"-"`
	XXX_unrecognized     []byte         `json:"-"`
	XXX_sizecache        int32          `json:"-"`
}

func (m *SharedMemoryControlResponse) Reset()         { *m = SharedMemoryControlResponse{} }
func (m *SharedMemoryControlResponse) String() string { return proto.CompactTextString(m) }
func (*SharedMemoryControlResponse) ProtoMessage()    {}
func (*SharedMemoryControlResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{10}
}

func (m *SharedMemoryControlResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SharedMemoryControlResponse.Unmarshal(m, b)
}
func (m *SharedMemoryControlResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SharedMemoryControlResponse.Marshal(b, m, deterministic)
}
func (m *SharedMemoryControlResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SharedMemoryControlResponse.Merge(m, src)
}
func (m *SharedMemoryControlResponse) XXX_Size() int {
	return xxx_messageInfo_SharedMemoryControlResponse.Size(m)
}
func (m *SharedMemoryControlResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_SharedMemoryControlResponse.DiscardUnknown(m)
}

var xxx_messageInfo_SharedMemoryControlResponse proto.InternalMessageInfo

func (m *SharedMemoryControlResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

//@@
//@@.. cpp:var:: message InferRequest
//@@
//@@   Request message for Infer gRPC endpoint.
//@@
type InferRequest struct {
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The name of the model to use for inferencing.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@  .. cpp:var:: int64 version
	//@@
	//@@     The version of the model to use for inference. If -1
	//@@     the latest/most-recent version of the model is used.
	//@@
	ModelVersion int64 `protobuf:"varint,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	//@@  .. cpp:var:: InferRequestHeader meta_data
	//@@
	//@@     Meta-data for the request profiling input tensors and requesting
	//@@     output tensors.
	//@@
	MetaData *InferRequestHeader `protobuf:"bytes,3,opt,name=meta_data,json=metaData,proto3" json:"meta_data,omitempty"`
	//@@  .. cpp:var:: bytes raw_input (repeated)
	//@@
	//@@     The raw input tensor data in the order specified in 'meta_data'.
	//@@
	RawInput             [][]byte `protobuf:"bytes,4,rep,name=raw_input,json=rawInput,proto3" json:"raw_input,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferRequest) Reset()         { *m = InferRequest{} }
func (m *InferRequest) String() string { return proto.CompactTextString(m) }
func (*InferRequest) ProtoMessage()    {}
func (*InferRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{11}
}

func (m *InferRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferRequest.Unmarshal(m, b)
}
func (m *InferRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferRequest.Marshal(b, m, deterministic)
}
func (m *InferRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferRequest.Merge(m, src)
}
func (m *InferRequest) XXX_Size() int {
	return xxx_messageInfo_InferRequest.Size(m)
}
func (m *InferRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_InferRequest.DiscardUnknown(m)
}

var xxx_messageInfo_InferRequest proto.InternalMessageInfo

func (m *InferRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *InferRequest) GetModelVersion() int64 {
	if m != nil {
		return m.ModelVersion
	}
	return 0
}

func (m *InferRequest) GetMetaData() *InferRequestHeader {
	if m != nil {
		return m.MetaData
	}
	return nil
}

func (m *InferRequest) GetRawInput() [][]byte {
	if m != nil {
		return m.RawInput
	}
	return nil
}

//@@
//@@.. cpp:var:: message InferResponse
//@@
//@@   Response message for Infer gRPC endpoint.
//@@
type InferResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	//@@  .. cpp:var:: InferResponseHeader meta_data
	//@@
	//@@     The response meta-data for the output tensors.
	//@@
	MetaData *InferResponseHeader `protobuf:"bytes,2,opt,name=meta_data,json=metaData,proto3" json:"meta_data,omitempty"`
	//@@  .. cpp:var:: bytes raw_output (repeated)
	//@@
	//@@     The raw output tensor data in the order specified in 'meta_data'.
	//@@
	RawOutput            [][]byte `protobuf:"bytes,3,rep,name=raw_output,json=rawOutput,proto3" json:"raw_output,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferResponse) Reset()         { *m = InferResponse{} }
func (m *InferResponse) String() string { return proto.CompactTextString(m) }
func (*InferResponse) ProtoMessage()    {}
func (*InferResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{12}
}

func (m *InferResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferResponse.Unmarshal(m, b)
}
func (m *InferResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferResponse.Marshal(b, m, deterministic)
}
func (m *InferResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferResponse.Merge(m, src)
}
func (m *InferResponse) XXX_Size() int {
	return xxx_messageInfo_InferResponse.Size(m)
}
func (m *InferResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_InferResponse.DiscardUnknown(m)
}

var xxx_messageInfo_InferResponse proto.InternalMessageInfo

func (m *InferResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

func (m *InferResponse) GetMetaData() *InferResponseHeader {
	if m != nil {
		return m.MetaData
	}
	return nil
}

func (m *InferResponse) GetRawOutput() [][]byte {
	if m != nil {
		return m.RawOutput
	}
	return nil
}

func init() {
	proto.RegisterEnum("nvidia.inferenceserver.ModelControlRequest_Type", ModelControlRequest_Type_name, ModelControlRequest_Type_value)
	proto.RegisterEnum("nvidia.inferenceserver.SharedMemoryControlRequest_Type", SharedMemoryControlRequest_Type_name, SharedMemoryControlRequest_Type_value)
	proto.RegisterType((*StatusRequest)(nil), "nvidia.inferenceserver.StatusRequest")
	proto.RegisterType((*StatusResponse)(nil), "nvidia.inferenceserver.StatusResponse")
	proto.RegisterType((*ProfileRequest)(nil), "nvidia.inferenceserver.ProfileRequest")
	proto.RegisterType((*ProfileResponse)(nil), "nvidia.inferenceserver.ProfileResponse")
	proto.RegisterType((*HealthRequest)(nil), "nvidia.inferenceserver.HealthRequest")
	proto.RegisterType((*HealthResponse)(nil), "nvidia.inferenceserver.HealthResponse")
	proto.RegisterType((*ModelControlRequest)(nil), "nvidia.inferenceserver.ModelControlRequest")
	proto.RegisterType((*ModelControlResponse)(nil), "nvidia.inferenceserver.ModelControlResponse")
	proto.RegisterType((*InferSharedMemoryRegion)(nil), "nvidia.inferenceserver.InferSharedMemoryRegion")
	proto.RegisterType((*SharedMemoryControlRequest)(nil), "nvidia.inferenceserver.SharedMemoryControlRequest")
	proto.RegisterType((*SharedMemoryControlResponse)(nil), "nvidia.inferenceserver.SharedMemoryControlResponse")
	proto.RegisterType((*InferRequest)(nil), "nvidia.inferenceserver.InferRequest")
	proto.RegisterType((*InferResponse)(nil), "nvidia.inferenceserver.InferResponse")
}

func init() { proto.RegisterFile("grpc_service.proto", fileDescriptor_626e658682f5c341) }

var fileDescriptor_626e658682f5c341 = []byte{
	// 760 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x56, 0xdf, 0x4e, 0x1a, 0x4d,
	0x14, 0x67, 0x05, 0x11, 0x0e, 0x7f, 0x3e, 0x32, 0x1a, 0x25, 0xeb, 0x67, 0x62, 0x46, 0xfd, 0x3e,
	0x52, 0x1b, 0x6a, 0xf0, 0xa2, 0xbd, 0x35, 0x6a, 0x94, 0x88, 0x7f, 0xb2, 0xa8, 0x89, 0x49, 0x93,
	0xed, 0x08, 0x07, 0xd9, 0xc8, 0xee, 0xd2, 0xd9, 0x05, 0x83, 0xbd, 0xe9, 0x55, 0x9f, 0xa1, 0xaf,
	0xd0, 0x37, 0xe8, 0x13, 0xf4, 0xa6, 0x2f, 0xd5, 0xcc, 0xec, 0x40, 0xd9, 0xca, 0x56, 0xbc, 0xe0,
	0x8a, 0x39, 0x67, 0xcf, 0xf9, 0xcd, 0xef, 0x77, 0xe6, 0xcc, 0x19, 0x80, 0xdc, 0xf1, 0x6e, 0xc3,
	0xf4, 0x90, 0xf7, 0xad, 0x06, 0x96, 0xbb, 0xdc, 0xf5, 0x5d, 0xb2, 0xec, 0xf4, 0xad, 0xa6, 0xc5,
	0xca, 0x96, 0xd3, 0x42, 0x8e, 0x4e, 0x03, 0xc5, 0x67, 0xe4, 0x7a, 0x9a, 0x75, 0xad, 0x20, 0x44,
	0x5f, 0xe2, 0xf8, 0xb1, 0x87, 0x9e, 0x6f, 0x7a, 0x3e, 0xf3, 0x7b, 0x9e, 0xf2, 0x2e, 0x06, 0x81,
	0x21, 0x27, 0x2d, 0x43, 0xae, 0x2e, 0x6d, 0x23, 0x48, 0x21, 0x6b, 0x00, 0xb6, 0xdb, 0xc4, 0x8e,
	0xe9, 0x30, 0x1b, 0x8b, 0xda, 0xba, 0x56, 0x4a, 0x1b, 0x69, 0xe9, 0x39, 0x63, 0x36, 0xd2, 0x6f,
	0x1a, 0xe4, 0x87, 0x09, 0x5e, 0xd7, 0x75, 0x3c, 0x24, 0x35, 0xc8, 0x87, 0xf7, 0x93, 0x59, 0x99,
	0xca, 0x56, 0x79, 0x32, 0xd3, 0xb2, 0xda, 0x4a, 0xc1, 0xe4, 0xf8, 0xb8, 0x49, 0xaa, 0x90, 0x0b,
	0xf1, 0x2c, 0xce, 0x49, 0xb0, 0xcd, 0x28, 0xb0, 0xba, 0xfc, 0x51, 0x58, 0x59, 0x6f, 0xcc, 0xa2,
	0x14, 0xf2, 0x17, 0xdc, 0x6d, 0x59, 0x1d, 0x1c, 0x8a, 0x2b, 0x40, 0xbc, 0x61, 0x37, 0x95, 0x2a,
	0xb1, 0xa4, 0x26, 0xfc, 0x33, 0x8a, 0x99, 0x85, 0x1e, 0xba, 0x01, 0xb9, 0x63, 0x64, 0x1d, 0xbf,
	0x3d, 0xe4, 0x40, 0x20, 0x21, 0xca, 0xa9, 0x48, 0xc8, 0x35, 0xed, 0x43, 0x7e, 0x18, 0x34, 0x93,
	0xa2, 0x2e, 0x43, 0xb2, 0x2d, 0xf1, 0x65, 0x35, 0x53, 0x86, 0xb2, 0xe8, 0x57, 0x0d, 0x16, 0x4f,
	0xc5, 0xd9, 0xee, 0xbb, 0x8e, 0xcf, 0xdd, 0xce, 0x74, 0x4d, 0x40, 0x0e, 0x20, 0xe1, 0x0f, 0xba,
	0x28, 0xc1, 0xf2, 0x95, 0x9d, 0x28, 0x4a, 0x13, 0x90, 0xcb, 0x97, 0x83, 0x2e, 0x1a, 0x32, 0x9b,
	0xfe, 0x0b, 0x09, 0x61, 0x11, 0x80, 0xe4, 0xd5, 0x59, 0xed, 0x7c, 0xef, 0xa0, 0x10, 0x23, 0x29,
	0x48, 0xc8, 0x95, 0x46, 0x9b, 0xb0, 0x14, 0xce, 0x9f, 0xc9, 0xe9, 0x7c, 0x82, 0x95, 0xaa, 0x88,
	0xaf, 0xb7, 0x19, 0xc7, 0xe6, 0x29, 0xda, 0x2e, 0x1f, 0x18, 0x78, 0x67, 0xb9, 0x8e, 0x38, 0xa7,
	0x31, 0xf5, 0x72, 0x4d, 0x56, 0x60, 0xc1, 0x6b, 0xdb, 0xe6, 0x3d, 0x0e, 0xa4, 0xf6, 0xb4, 0x91,
	0xf4, 0xda, 0xf6, 0x09, 0x0e, 0x44, 0x81, 0xdd, 0x56, 0xcb, 0x43, 0xbf, 0x18, 0x5f, 0xd7, 0x4a,
	0x09, 0x43, 0x59, 0x64, 0x15, 0xd2, 0xb7, 0x03, 0x1f, 0x4d, 0xcf, 0x7a, 0xc4, 0x62, 0x42, 0x7e,
	0x4a, 0x09, 0x47, 0xdd, 0x7a, 0x44, 0xfa, 0x65, 0x0e, 0xf4, 0xf1, 0x8d, 0xff, 0x38, 0x84, 0x13,
	0x55, 0x65, 0x4d, 0x56, 0xf9, 0x6d, 0xe4, 0x05, 0x88, 0x44, 0x18, 0x2b, 0x36, 0x61, 0xb0, 0xe4,
	0xc9, 0x40, 0xd3, 0x96, 0x91, 0x26, 0x97, 0x2a, 0xd5, 0xed, 0x7a, 0x13, 0x05, 0x1e, 0x51, 0x1c,
	0x83, 0x78, 0x4f, 0x7c, 0xf4, 0x9d, 0x3a, 0xcf, 0x2c, 0xa4, 0x8c, 0xc3, 0xa3, 0x6a, 0xfd, 0xf2,
	0xd0, 0x28, 0xc4, 0x48, 0x1e, 0xe0, 0xea, 0x6c, 0x64, 0x6b, 0x84, 0x40, 0xfe, 0xb7, 0x6d, 0xee,
	0xd5, 0x6a, 0x85, 0x39, 0x7a, 0x0f, 0xab, 0x13, 0x55, 0xcc, 0xe4, 0xc8, 0xbf, 0x6b, 0x90, 0x95,
	0xb2, 0xa6, 0x6c, 0xf6, 0x0d, 0xc8, 0x05, 0x9f, 0xfb, 0xc8, 0xbd, 0x61, 0xc9, 0xe2, 0x46, 0x56,
	0x3a, 0xaf, 0x03, 0x1f, 0x39, 0x82, 0xb4, 0x8d, 0x3e, 0x33, 0x9b, 0xcc, 0x67, 0xb2, 0x05, 0x32,
	0x95, 0x57, 0x7f, 0xad, 0xa9, 0xda, 0xfc, 0x18, 0x59, 0x13, 0xb9, 0x91, 0x12, 0xc9, 0x07, 0xcc,
	0x67, 0xa2, 0x61, 0x38, 0x7b, 0x30, 0x2d, 0xa7, 0xdb, 0xf3, 0x8b, 0x89, 0xf5, 0x78, 0x29, 0x6b,
	0xa4, 0x38, 0x7b, 0xa8, 0x0a, 0x9b, 0xfe, 0xd0, 0x20, 0xa7, 0xb2, 0x67, 0x32, 0x26, 0x8e, 0xc7,
	0x55, 0x04, 0x9d, 0xb1, 0xfd, 0x8c, 0x8a, 0x80, 0xc7, 0x13, 0x19, 0x6b, 0x00, 0x42, 0x86, 0xdb,
	0xf3, 0x85, 0x8e, 0xb8, 0xd4, 0x21, 0x84, 0x9d, 0x4b, 0x47, 0xe5, 0xe7, 0x3c, 0x64, 0x8e, 0x8c,
	0x8b, 0xfd, 0x7a, 0xf0, 0xb2, 0x91, 0x1b, 0x48, 0x2a, 0x0a, 0x91, 0xc4, 0x43, 0xaf, 0x94, 0xfe,
	0xdf, 0x73, 0x61, 0x01, 0x2f, 0x1a, 0x23, 0xef, 0x61, 0x41, 0x0d, 0x78, 0x12, 0x99, 0x14, 0x7e,
	0x25, 0xf4, 0xff, 0x9f, 0x8d, 0x1b, 0xa1, 0xdf, 0x40, 0x32, 0x18, 0xdc, 0xd1, 0xc4, 0x43, 0xd3,
	0x3f, 0x9a, 0x78, 0x78, 0xfe, 0xd3, 0x18, 0xb9, 0x87, 0xec, 0xf8, 0x00, 0x24, 0xdb, 0x2f, 0x18,
	0xb3, 0xfa, 0xeb, 0xe9, 0x82, 0x47, 0x9b, 0x7d, 0xd6, 0x60, 0x71, 0xc2, 0x15, 0x24, 0x95, 0x97,
	0x4f, 0x1d, 0x7d, 0xf7, 0x45, 0x39, 0x23, 0x0a, 0xd7, 0x30, 0x2f, 0x7b, 0x8a, 0x6c, 0x4e, 0x73,
	0x71, 0xf4, 0xad, 0xa9, 0x1a, 0x93, 0xc6, 0xc8, 0x07, 0xc8, 0xd4, 0x7d, 0x8e, 0xcc, 0x9e, 0x05,
	0x7a, 0x49, 0xdb, 0xd1, 0x6e, 0x93, 0xf2, 0xaf, 0xd4, 0xee, 0xaf, 0x00, 0x00, 0x00, 0xff, 0xff,
	0x27, 0x23, 0xd6, 0x00, 0xae, 0x09, 0x00, 0x00,
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// GRPCServiceClient is the client API for GRPCService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type GRPCServiceClient interface {
	//@@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
	//@@
	//@@     Get status for entire inference server or for a specified model.
	//@@
	Status(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error)
	//@@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
	//@@
	//@@     Enable and disable low-level GPU profiling.
	//@@
	Profile(ctx context.Context, in *ProfileRequest, opts ...grpc.CallOption) (*ProfileResponse, error)
	//@@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
	//@@
	//@@     Check liveness and readiness of the inference server.
	//@@
	Health(ctx context.Context, in *HealthRequest, opts ...grpc.CallOption) (*HealthResponse, error)
	//@@  .. cpp:var:: rpc ModelControl(ModelControlRequest) returns
	//@@     (ModelControlResponse)
	//@@
	//@@     Request to load / unload a specified model.
	//@@
	ModelControl(ctx context.Context, in *ModelControlRequest, opts ...grpc.CallOption) (*ModelControlResponse, error)
	//@@  .. cpp:var:: rpc SharedMemoryControl(SharedMemoryControlRequest) returns
	//@@     (SharedMemoryControlResponse)
	//@@
	//@@     Request to register / unregister a specified shared memory region.
	//@@
	SharedMemoryControl(ctx context.Context, in *SharedMemoryControlRequest, opts ...grpc.CallOption) (*SharedMemoryControlResponse, error)
	//@@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
	//@@
	//@@     Request inference using a specific model. [ To handle large input
	//@@     tensors likely need to set the maximum message size to that they
	//@@     can be transmitted in one pass.
	//@@
	Infer(ctx context.Context, in *InferRequest, opts ...grpc.CallOption) (*InferResponse, error)
	//@@  .. cpp:var:: rpc StreamInfer(stream InferRequest) returns (stream
	//@@     InferResponse)
	//@@
	//@@     Request inferences using a specific model in a streaming manner.
	//@@     Individual inference requests sent through the same stream will be
	//@@     processed in order and be returned on completion
	//@@
	StreamInfer(ctx context.Context, opts ...grpc.CallOption) (GRPCService_StreamInferClient, error)
}

type gRPCServiceClient struct {
	cc *grpc.ClientConn
}

func NewGRPCServiceClient(cc *grpc.ClientConn) GRPCServiceClient {
	return &gRPCServiceClient{cc}
}

func (c *gRPCServiceClient) Status(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error) {
	out := new(StatusResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Status", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) Profile(ctx context.Context, in *ProfileRequest, opts ...grpc.CallOption) (*ProfileResponse, error) {
	out := new(ProfileResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Profile", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) Health(ctx context.Context, in *HealthRequest, opts ...grpc.CallOption) (*HealthResponse, error) {
	out := new(HealthResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Health", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) ModelControl(ctx context.Context, in *ModelControlRequest, opts ...grpc.CallOption) (*ModelControlResponse, error) {
	out := new(ModelControlResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/ModelControl", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) SharedMemoryControl(ctx context.Context, in *SharedMemoryControlRequest, opts ...grpc.CallOption) (*SharedMemoryControlResponse, error) {
	out := new(SharedMemoryControlResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/SharedMemoryControl", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) Infer(ctx context.Context, in *InferRequest, opts ...grpc.CallOption) (*InferResponse, error) {
	out := new(InferResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Infer", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) StreamInfer(ctx context.Context, opts ...grpc.CallOption) (GRPCService_StreamInferClient, error) {
	stream, err := c.cc.NewStream(ctx, &_GRPCService_serviceDesc.Streams[0], "/nvidia.inferenceserver.GRPCService/StreamInfer", opts...)
	if err != nil {
		return nil, err
	}
	x := &gRPCServiceStreamInferClient{stream}
	return x, nil
}

type GRPCService_StreamInferClient interface {
	Send(*InferRequest) error
	Recv() (*InferResponse, error)
	grpc.ClientStream
}

type gRPCServiceStreamInferClient struct {
	grpc.ClientStream
}

func (x *gRPCServiceStreamInferClient) Send(m *InferRequest) error {
	return x.ClientStream.SendMsg(m)
}

func (x *gRPCServiceStreamInferClient) Recv() (*InferResponse, error) {
	m := new(InferResponse)
	if err := x.ClientStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

// GRPCServiceServer is the server API for GRPCService service.
type GRPCServiceServer interface {
	//@@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
	//@@
	//@@     Get status for entire inference server or for a specified model.
	//@@
	Status(context.Context, *StatusRequest) (*StatusResponse, error)
	//@@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
	//@@
	//@@     Enable and disable low-level GPU profiling.
	//@@
	Profile(context.Context, *ProfileRequest) (*ProfileResponse, error)
	//@@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
	//@@
	//@@     Check liveness and readiness of the inference server.
	//@@
	Health(context.Context, *HealthRequest) (*HealthResponse, error)
	//@@  .. cpp:var:: rpc ModelControl(ModelControlRequest) returns
	//@@     (ModelControlResponse)
	//@@
	//@@     Request to load / unload a specified model.
	//@@
	ModelControl(context.Context, *ModelControlRequest) (*ModelControlResponse, error)
	//@@  .. cpp:var:: rpc SharedMemoryControl(SharedMemoryControlRequest) returns
	//@@     (SharedMemoryControlResponse)
	//@@
	//@@     Request to register / unregister a specified shared memory region.
	//@@
	SharedMemoryControl(context.Context, *SharedMemoryControlRequest) (*SharedMemoryControlResponse, error)
	//@@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
	//@@
	//@@     Request inference using a specific model. [ To handle large input
	//@@     tensors likely need to set the maximum message size to that they
	//@@     can be transmitted in one pass.
	//@@
	Infer(context.Context, *InferRequest) (*InferResponse, error)
	//@@  .. cpp:var:: rpc StreamInfer(stream InferRequest) returns (stream
	//@@     InferResponse)
	//@@
	//@@     Request inferences using a specific model in a streaming manner.
	//@@     Individual inference requests sent through the same stream will be
	//@@     processed in order and be returned on completion
	//@@
	StreamInfer(GRPCService_StreamInferServer) error
}

// UnimplementedGRPCServiceServer can be embedded to have forward compatible implementations.
type UnimplementedGRPCServiceServer struct {
}

func (*UnimplementedGRPCServiceServer) Status(ctx context.Context, req *StatusRequest) (*StatusResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Status not implemented")
}
func (*UnimplementedGRPCServiceServer) Profile(ctx context.Context, req *ProfileRequest) (*ProfileResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Profile not implemented")
}
func (*UnimplementedGRPCServiceServer) Health(ctx context.Context, req *HealthRequest) (*HealthResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Health not implemented")
}
func (*UnimplementedGRPCServiceServer) ModelControl(ctx context.Context, req *ModelControlRequest) (*ModelControlResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ModelControl not implemented")
}
func (*UnimplementedGRPCServiceServer) SharedMemoryControl(ctx context.Context, req *SharedMemoryControlRequest) (*SharedMemoryControlResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method SharedMemoryControl not implemented")
}
func (*UnimplementedGRPCServiceServer) Infer(ctx context.Context, req *InferRequest) (*InferResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Infer not implemented")
}
func (*UnimplementedGRPCServiceServer) StreamInfer(srv GRPCService_StreamInferServer) error {
	return status.Errorf(codes.Unimplemented, "method StreamInfer not implemented")
}

func RegisterGRPCServiceServer(s *grpc.Server, srv GRPCServiceServer) {
	s.RegisterService(&_GRPCService_serviceDesc, srv)
}

func _GRPCService_Status_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(StatusRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Status(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Status",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Status(ctx, req.(*StatusRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_Profile_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ProfileRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Profile(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Profile",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Profile(ctx, req.(*ProfileRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_Health_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(HealthRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Health(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Health",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Health(ctx, req.(*HealthRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_ModelControl_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelControlRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).ModelControl(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/ModelControl",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).ModelControl(ctx, req.(*ModelControlRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_SharedMemoryControl_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SharedMemoryControlRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).SharedMemoryControl(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/SharedMemoryControl",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).SharedMemoryControl(ctx, req.(*SharedMemoryControlRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_Infer_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(InferRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Infer(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Infer",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Infer(ctx, req.(*InferRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_StreamInfer_Handler(srv interface{}, stream grpc.ServerStream) error {
	return srv.(GRPCServiceServer).StreamInfer(&gRPCServiceStreamInferServer{stream})
}

type GRPCService_StreamInferServer interface {
	Send(*InferResponse) error
	Recv() (*InferRequest, error)
	grpc.ServerStream
}

type gRPCServiceStreamInferServer struct {
	grpc.ServerStream
}

func (x *gRPCServiceStreamInferServer) Send(m *InferResponse) error {
	return x.ServerStream.SendMsg(m)
}

func (x *gRPCServiceStreamInferServer) Recv() (*InferRequest, error) {
	m := new(InferRequest)
	if err := x.ServerStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

var _GRPCService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "nvidia.inferenceserver.GRPCService",
	HandlerType: (*GRPCServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Status",
			Handler:    _GRPCService_Status_Handler,
		},
		{
			MethodName: "Profile",
			Handler:    _GRPCService_Profile_Handler,
		},
		{
			MethodName: "Health",
			Handler:    _GRPCService_Health_Handler,
		},
		{
			MethodName: "ModelControl",
			Handler:    _GRPCService_ModelControl_Handler,
		},
		{
			MethodName: "SharedMemoryControl",
			Handler:    _GRPCService_SharedMemoryControl_Handler,
		},
		{
			MethodName: "Infer",
			Handler:    _GRPCService_Infer_Handler,
		},
	},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "StreamInfer",
			Handler:       _GRPCService_StreamInfer_Handler,
			ServerStreams: true,
			ClientStreams: true,
		},
	},
	Metadata: "grpc_service.proto",
}
